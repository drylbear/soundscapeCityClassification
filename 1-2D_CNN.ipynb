{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D,Dropout, Reshape, LSTM, BatchNormalization,TimeDistributed\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam #, RMSprop, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from time import time\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  16 #multiple of 1290/5 (recording length)\n",
    "nb_classes = 6\n",
    "rows, cols = 431, 128   \n",
    "nb_epoch = 200\n",
    "pool_size = (5,5)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.3                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.3              # drop probability for dropout @ fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data sorted\n"
     ]
    }
   ],
   "source": [
    "#might need to rewrite these\n",
    "\n",
    "\n",
    "\n",
    "def fileLists():\n",
    "    trainlist=[]\n",
    "    validationlist=[]\n",
    "    testlist=[]\n",
    "    evalSetupFiles='..\\\\..\\\\CASAdatasets\\\\DCASE18_ASCT1\\\\TUT-urban-acoustic-scenes-2018-development\\\\evaluation_setup\\\\*.txt'\n",
    "    txtfilelist=glob.glob(evalSetupFiles)\n",
    "    for txt in txtfilelist:\n",
    "        if '_location' not in txt:\n",
    "            continue\n",
    "        with open(txt,'r') as evaltxtfile:\n",
    "            for line in evaltxtfile.readlines():\n",
    "                line=line.strip().split('\\t')[0]\n",
    "                if 'train' in txt:\n",
    "                    trainlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "                elif 'test' in txt:\n",
    "                    testlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "                else:\n",
    "                    validationlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "    print('trainfiles: ', str(len(trainlist)))\n",
    "    print('validationfiles: ', str(len(validationlist)))\n",
    "    print('testfiles: ', str(len(testlist)))\n",
    "    return trainlist,validationlist,testlist\n",
    "\n",
    "labelRef={'barcelona':0, 'helsinki':1, 'london':2, 'paris':3, 'stockholm':4, 'vienna':5}\n",
    "\n",
    "def getData(flist):\n",
    "    pth='..\\\\..\\\\CASAdatasets\\\\DCASE18_ASCT1\\\\TUT-urban-acoustic-scenes-2018-development\\\\'\n",
    "    X_=np.zeros(((len(flist)),128,431))\n",
    "    Y_=np.zeros(((len(flist)),nb_classes))\n",
    "    for i,tfile in enumerate(flist):\n",
    "        with open(pth+tfile, \"rb\" ) as scenesample:\n",
    "            fv=pickle.load(scenesample)\n",
    "        X_[i,:,:]=fv\n",
    "        location=tfile.split('-')[1]\n",
    "        Y_[i,labelRef[location]]=1\n",
    "    return X_, Y_\n",
    "#trainlist,validationlist,testlist=fileLists()\n",
    "#X_train, Y_train = getData(trainlist)\n",
    "#X_val, Y_val = getData(validationlist)\n",
    "#X_test, Y_test = getData(testlist)\n",
    "\n",
    "print(' data sorted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 128, 431, 32)      1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 216, 64)       100416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64, 216, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 108, 128)      32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 54, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 110592)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                7077952   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 7,214,406\n",
      "Trainable params: 7,213,830\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###### Convolutional model\n",
    "def compileCRNN(cols,rows,nb_classes=1):\n",
    "\n",
    "    model = Sequential()\n",
    "    # conv1 layer\n",
    "    model.add(Conv2D(32, (7, 7), padding='same', activation='relu', input_shape=(cols,rows,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv2 layer\n",
    "    model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(4,7), strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv3 layer\n",
    "    model.add(Conv2D(128, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # fc1 layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(prob_drop_hidden))\n",
    "    model.add(BatchNormalization())\n",
    "   \n",
    "    # fc2 layer\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = compileCRNN(cols,rows,nb_classes=nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "#fold=1\n",
    "def train_network(model, model_name, X_train, Y_train, X_val, Y_val, nb_epoch, validationsplit_size, batchsize, early_stoping_patience, output_folder):\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder,model_name + '.hdf5'),save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stoping_patience)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(time()))\n",
    "    Callbacks=[checkpointer,  tensorboard] #early_stopping,\n",
    "  #  print(samples)\n",
    "    steps=int(samples/batchsize)\n",
    "    validationsteps=int(validationsplit_size/batchsize)\n",
    "    history = model.fit(X_train, Y_train, epochs=nb_epoch, callbacks=Callbacks, batch_size=batch_size, validation_data=(X_val, Y_val), shuffle=True, verbose=1)\n",
    "    return history,model\n",
    "\n",
    "def buildModel(savemodelfilename, samples,model,X_train, Y_train,X_val, Y_val):\n",
    "    valSplit_size = int(samples/4)\n",
    "    early_stoping_patience=5\n",
    "    history,model = train_network(model, 'models\\\\best_model_simple', X_train, Y_train, X_val, Y_val, nb_epoch, valSplit_size, batch_size, early_stoping_patience,'.')\n",
    "    model.save_weights(savemodelfilename)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainfiles:  6122\n",
      "validationfiles:  2518\n",
      "testfiles:  2518\n",
      "lists sorted\n",
      "data obtained\n",
      "(6122, 128, 431, 1) (2518, 128, 431, 1) (2518, 128, 431, 1)\n",
      "data sorted\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 128, 431, 32)      1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 64, 216, 64)       100416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 64, 216, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 108, 128)      32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 16, 54, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 110592)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                7077952   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 7,214,406\n",
      "Trainable params: 7,213,830\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "model compiled\n",
      "2019-04-05 15:40:04.719746\n",
      "Train on 6122 samples, validate on 2518 samples\n",
      "Epoch 1/200\n",
      "6122/6122 [==============================] - 26s 4ms/step - loss: 1.8574 - acc: 0.2320 - val_loss: 2.3703 - val_acc: 0.1493\n",
      "Epoch 2/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.7176 - acc: 0.2605 - val_loss: 3.1351 - val_acc: 0.1493\n",
      "Epoch 3/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.7525 - acc: 0.2369 - val_loss: 2.0680 - val_acc: 0.1966\n",
      "Epoch 4/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.6711 - acc: 0.2970 - val_loss: 2.9753 - val_acc: 0.2172\n",
      "Epoch 5/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.5678 - acc: 0.3600 - val_loss: 2.8650 - val_acc: 0.1922\n",
      "Epoch 6/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.4579 - acc: 0.4144 - val_loss: 6.2433 - val_acc: 0.2303\n",
      "Epoch 7/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.4085 - acc: 0.4440 - val_loss: 12.8915 - val_acc: 0.1807\n",
      "Epoch 8/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.3839 - acc: 0.4566 - val_loss: 2.4559 - val_acc: 0.2816\n",
      "Epoch 9/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.3419 - acc: 0.4779 - val_loss: 1.8936 - val_acc: 0.3205\n",
      "Epoch 10/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.4436 - acc: 0.4216 - val_loss: 1.8541 - val_acc: 0.1882\n",
      "Epoch 11/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.3296 - acc: 0.4762 - val_loss: 11.5444 - val_acc: 0.1716\n",
      "Epoch 12/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.2055 - acc: 0.5346 - val_loss: 11.8721 - val_acc: 0.2077\n",
      "Epoch 13/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 1.1525 - acc: 0.5554 - val_loss: 12.1851 - val_acc: 0.2153\n",
      "Epoch 14/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.1448 - acc: 0.5555 - val_loss: 3.2105 - val_acc: 0.1894\n",
      "Epoch 15/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.0334 - acc: 0.6011 - val_loss: 9.0619 - val_acc: 0.1875\n",
      "Epoch 16/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.0299 - acc: 0.5996 - val_loss: 5.4693 - val_acc: 0.1859\n",
      "Epoch 17/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.9525 - acc: 0.6423 - val_loss: 6.2073 - val_acc: 0.2133\n",
      "Epoch 18/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.9776 - acc: 0.6284 - val_loss: 1.8471 - val_acc: 0.3761\n",
      "Epoch 19/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.9568 - acc: 0.6419 - val_loss: 2.1060 - val_acc: 0.3082\n",
      "Epoch 20/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8891 - acc: 0.6632 - val_loss: 6.8238 - val_acc: 0.2029\n",
      "Epoch 21/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8698 - acc: 0.6767 - val_loss: 2.3900 - val_acc: 0.2867\n",
      "Epoch 22/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8331 - acc: 0.6963 - val_loss: 7.8313 - val_acc: 0.2168\n",
      "Epoch 23/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8006 - acc: 0.7110 - val_loss: 12.7809 - val_acc: 0.1775\n",
      "Epoch 24/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8021 - acc: 0.7021 - val_loss: 12.3655 - val_acc: 0.2153\n",
      "Epoch 25/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8932 - acc: 0.6632 - val_loss: 12.2719 - val_acc: 0.1902\n",
      "Epoch 26/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7867 - acc: 0.7060 - val_loss: 6.4893 - val_acc: 0.2105\n",
      "Epoch 27/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7392 - acc: 0.7303 - val_loss: 2.4423 - val_acc: 0.2701\n",
      "Epoch 28/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7080 - acc: 0.7519 - val_loss: 5.5451 - val_acc: 0.2796\n",
      "Epoch 29/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6438 - acc: 0.7687 - val_loss: 4.7164 - val_acc: 0.3836\n",
      "Epoch 30/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6291 - acc: 0.7761 - val_loss: 8.1148 - val_acc: 0.2847\n",
      "Epoch 31/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.6054 - acc: 0.7839 - val_loss: 6.9764 - val_acc: 0.2708\n",
      "Epoch 32/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5956 - acc: 0.7947 - val_loss: 12.3271 - val_acc: 0.1906\n",
      "Epoch 33/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6864 - acc: 0.7468 - val_loss: 1.6797 - val_acc: 0.4996\n",
      "Epoch 34/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6270 - acc: 0.7824 - val_loss: 4.6477 - val_acc: 0.3352\n",
      "Epoch 35/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.6497 - acc: 0.7627 - val_loss: 12.2438 - val_acc: 0.2017\n",
      "Epoch 36/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.7496 - acc: 0.7225 - val_loss: 12.2851 - val_acc: 0.1898\n",
      "Epoch 37/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6336 - acc: 0.7783 - val_loss: 4.4166 - val_acc: 0.3558\n",
      "Epoch 38/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6750 - acc: 0.7440 - val_loss: 7.0385 - val_acc: 0.3010\n",
      "Epoch 39/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7139 - acc: 0.7316 - val_loss: 4.9907 - val_acc: 0.3407\n",
      "Epoch 40/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6063 - acc: 0.7880 - val_loss: 7.3526 - val_acc: 0.3030\n",
      "Epoch 41/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5648 - acc: 0.8028 - val_loss: 7.0567 - val_acc: 0.3257\n",
      "Epoch 42/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5704 - acc: 0.8038 - val_loss: 6.7594 - val_acc: 0.3463\n",
      "Epoch 43/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5402 - acc: 0.8189 - val_loss: 9.8818 - val_acc: 0.2506\n",
      "Epoch 44/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5105 - acc: 0.8254 - val_loss: 11.4127 - val_acc: 0.1612\n",
      "Epoch 45/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5834 - acc: 0.7929 - val_loss: 4.6482 - val_acc: 0.3479\n",
      "Epoch 46/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4984 - acc: 0.8285 - val_loss: 3.6746 - val_acc: 0.3872\n",
      "Epoch 47/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5564 - acc: 0.8007 - val_loss: 3.3552 - val_acc: 0.3217\n",
      "Epoch 48/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4812 - acc: 0.8278 - val_loss: 5.7033 - val_acc: 0.4031\n",
      "Epoch 49/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4320 - acc: 0.8551 - val_loss: 4.5382 - val_acc: 0.3832\n",
      "Epoch 50/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4324 - acc: 0.8491 - val_loss: 5.9951 - val_acc: 0.3419\n",
      "Epoch 51/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4464 - acc: 0.8484 - val_loss: 4.4964 - val_acc: 0.3483\n",
      "Epoch 52/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4242 - acc: 0.8510 - val_loss: 3.6151 - val_acc: 0.4214\n",
      "Epoch 53/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3824 - acc: 0.8648 - val_loss: 11.7590 - val_acc: 0.2149\n",
      "Epoch 54/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4051 - acc: 0.8603 - val_loss: 4.2880 - val_acc: 0.2915\n",
      "Epoch 55/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3804 - acc: 0.8728 - val_loss: 4.2613 - val_acc: 0.3257\n",
      "Epoch 56/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3782 - acc: 0.8688 - val_loss: 2.7880 - val_acc: 0.3904\n",
      "Epoch 57/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5688 - acc: 0.7955 - val_loss: 3.0377 - val_acc: 0.4166\n",
      "Epoch 58/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.4333 - acc: 0.8456 - val_loss: 4.4765 - val_acc: 0.4142\n",
      "Epoch 59/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.4318 - acc: 0.8483 - val_loss: 4.2650 - val_acc: 0.3844\n",
      "Epoch 60/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.3821 - acc: 0.8692 - val_loss: 4.5792 - val_acc: 0.3689\n",
      "Epoch 61/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3707 - acc: 0.8768 - val_loss: 2.8232 - val_acc: 0.3554\n",
      "Epoch 62/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3550 - acc: 0.8871 - val_loss: 4.6852 - val_acc: 0.3670\n",
      "Epoch 63/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3480 - acc: 0.8824 - val_loss: 2.5182 - val_acc: 0.3971\n",
      "Epoch 64/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.3076 - acc: 0.8981 - val_loss: 2.6240 - val_acc: 0.4635\n",
      "Epoch 65/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.3630 - acc: 0.8759 - val_loss: 6.3698 - val_acc: 0.3034\n",
      "Epoch 66/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.3709 - acc: 0.8662 - val_loss: 3.5056 - val_acc: 0.4106\n",
      "Epoch 67/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5617 - acc: 0.8002 - val_loss: 3.5532 - val_acc: 0.2971\n",
      "Epoch 68/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.3727 - acc: 0.8719 - val_loss: 2.9986 - val_acc: 0.4440\n",
      "Epoch 69/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.3277 - acc: 0.8956 - val_loss: 4.5279 - val_acc: 0.3523\n",
      "Epoch 70/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.3417 - acc: 0.8912 - val_loss: 8.3073 - val_acc: 0.2863\n",
      "Epoch 71/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3295 - acc: 0.8878 - val_loss: 4.1686 - val_acc: 0.3967\n",
      "Epoch 72/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2977 - acc: 0.9035 - val_loss: 3.0233 - val_acc: 0.3940\n",
      "Epoch 73/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2631 - acc: 0.9110 - val_loss: 3.3269 - val_acc: 0.4106\n",
      "Epoch 74/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2574 - acc: 0.9159 - val_loss: 2.8825 - val_acc: 0.4055\n",
      "Epoch 75/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3511 - acc: 0.8754 - val_loss: 3.6065 - val_acc: 0.3602\n",
      "Epoch 76/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3378 - acc: 0.8809 - val_loss: 6.9714 - val_acc: 0.2593\n",
      "Epoch 77/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2957 - acc: 0.8995 - val_loss: 3.6164 - val_acc: 0.4055\n",
      "Epoch 78/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3056 - acc: 0.8963 - val_loss: 4.7182 - val_acc: 0.4095\n",
      "Epoch 79/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2740 - acc: 0.9131 - val_loss: 4.6039 - val_acc: 0.3888\n",
      "Epoch 80/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2405 - acc: 0.9213 - val_loss: 6.8378 - val_acc: 0.2764\n",
      "Epoch 81/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2492 - acc: 0.9191 - val_loss: 2.8153 - val_acc: 0.4349\n",
      "Epoch 82/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2467 - acc: 0.9191 - val_loss: 8.7502 - val_acc: 0.2669\n",
      "Epoch 83/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3322 - acc: 0.8822 - val_loss: 9.0014 - val_acc: 0.2125\n",
      "Epoch 84/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3097 - acc: 0.8915 - val_loss: 3.8912 - val_acc: 0.3936\n",
      "Epoch 85/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2878 - acc: 0.9010 - val_loss: 3.6211 - val_acc: 0.4007\n",
      "Epoch 86/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2622 - acc: 0.9097 - val_loss: 3.9384 - val_acc: 0.4837\n",
      "Epoch 87/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2630 - acc: 0.9103 - val_loss: 3.7037 - val_acc: 0.4738\n",
      "Epoch 88/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2533 - acc: 0.9147 - val_loss: 3.4748 - val_acc: 0.4972\n",
      "Epoch 89/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2438 - acc: 0.9180 - val_loss: 3.0061 - val_acc: 0.5020\n",
      "Epoch 90/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2471 - acc: 0.9170 - val_loss: 4.6282 - val_acc: 0.3864\n",
      "Epoch 91/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2111 - acc: 0.9270 - val_loss: 7.7683 - val_acc: 0.2931\n",
      "Epoch 92/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.3644 - acc: 0.8750 - val_loss: 2.3469 - val_acc: 0.4932\n",
      "Epoch 93/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2936 - acc: 0.9012 - val_loss: 3.4078 - val_acc: 0.3678\n",
      "Epoch 94/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2670 - acc: 0.9085 - val_loss: 2.8006 - val_acc: 0.4635\n",
      "Epoch 95/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2460 - acc: 0.9126 - val_loss: 2.8589 - val_acc: 0.5099\n",
      "Epoch 96/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2184 - acc: 0.9262 - val_loss: 3.0471 - val_acc: 0.5409\n",
      "Epoch 97/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2147 - acc: 0.9275 - val_loss: 6.2214 - val_acc: 0.4317\n",
      "Epoch 98/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2003 - acc: 0.9347 - val_loss: 2.7209 - val_acc: 0.5175\n",
      "Epoch 99/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2304 - acc: 0.9206 - val_loss: 3.8507 - val_acc: 0.3892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2030 - acc: 0.9325 - val_loss: 7.4399 - val_acc: 0.3646\n",
      "Epoch 101/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2161 - acc: 0.9268 - val_loss: 5.2097 - val_acc: 0.4786\n",
      "Epoch 102/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1885 - acc: 0.9342 - val_loss: 7.5809 - val_acc: 0.3320\n",
      "Epoch 103/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2671 - acc: 0.9090 - val_loss: 3.3762 - val_acc: 0.4746\n",
      "Epoch 104/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2294 - acc: 0.9187 - val_loss: 4.7131 - val_acc: 0.4063\n",
      "Epoch 105/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1903 - acc: 0.9352 - val_loss: 3.3515 - val_acc: 0.5234\n",
      "Epoch 106/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1611 - acc: 0.9499 - val_loss: 6.3738 - val_acc: 0.3368\n",
      "Epoch 107/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1446 - acc: 0.9539 - val_loss: 3.5732 - val_acc: 0.4702\n",
      "Epoch 108/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1565 - acc: 0.9477 - val_loss: 5.2558 - val_acc: 0.4285\n",
      "Epoch 109/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1543 - acc: 0.9477 - val_loss: 2.8424 - val_acc: 0.4448\n",
      "Epoch 110/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2063 - acc: 0.9285 - val_loss: 2.8892 - val_acc: 0.5250\n",
      "Epoch 111/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1609 - acc: 0.9450 - val_loss: 2.7888 - val_acc: 0.5107\n",
      "Epoch 112/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1501 - acc: 0.9481 - val_loss: 3.3579 - val_acc: 0.5405\n",
      "Epoch 113/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1727 - acc: 0.9428 - val_loss: 2.8454 - val_acc: 0.4790\n",
      "Epoch 114/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1656 - acc: 0.9420 - val_loss: 5.0976 - val_acc: 0.4730\n",
      "Epoch 115/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1584 - acc: 0.9489 - val_loss: 2.8030 - val_acc: 0.4873\n",
      "Epoch 116/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1665 - acc: 0.9410 - val_loss: 2.2932 - val_acc: 0.5203\n",
      "Epoch 117/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1821 - acc: 0.9361 - val_loss: 3.4083 - val_acc: 0.5087\n",
      "Epoch 118/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1373 - acc: 0.9546 - val_loss: 2.9272 - val_acc: 0.5516\n",
      "Epoch 119/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1353 - acc: 0.9552 - val_loss: 2.1322 - val_acc: 0.5671\n",
      "Epoch 120/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.2457 - acc: 0.9182 - val_loss: 2.0266 - val_acc: 0.5612\n",
      "Epoch 121/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1399 - acc: 0.9523 - val_loss: 3.0949 - val_acc: 0.4253\n",
      "Epoch 122/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1629 - acc: 0.9467 - val_loss: 3.0460 - val_acc: 0.5135\n",
      "Epoch 123/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1528 - acc: 0.9513 - val_loss: 2.6128 - val_acc: 0.4833\n",
      "Epoch 124/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1420 - acc: 0.9492 - val_loss: 4.4912 - val_acc: 0.3531\n",
      "Epoch 125/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1746 - acc: 0.9446 - val_loss: 2.3118 - val_acc: 0.5056\n",
      "Epoch 126/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1646 - acc: 0.9456 - val_loss: 2.2628 - val_acc: 0.5361\n",
      "Epoch 127/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2759 - acc: 0.9046 - val_loss: 3.0940 - val_acc: 0.4230\n",
      "Epoch 128/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1739 - acc: 0.9428 - val_loss: 3.2126 - val_acc: 0.4416\n",
      "Epoch 129/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1574 - acc: 0.9476 - val_loss: 2.5804 - val_acc: 0.5127\n",
      "Epoch 130/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1761 - acc: 0.9412 - val_loss: 3.5849 - val_acc: 0.4976\n",
      "Epoch 131/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1704 - acc: 0.9414 - val_loss: 2.6495 - val_acc: 0.4972\n",
      "Epoch 132/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1470 - acc: 0.9497 - val_loss: 4.0663 - val_acc: 0.5087\n",
      "Epoch 133/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2436 - acc: 0.9190 - val_loss: 2.3039 - val_acc: 0.5286\n",
      "Epoch 134/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1493 - acc: 0.9497 - val_loss: 2.9937 - val_acc: 0.4420\n",
      "Epoch 135/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1906 - acc: 0.9350 - val_loss: 2.4646 - val_acc: 0.5175\n",
      "Epoch 136/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1555 - acc: 0.9482 - val_loss: 5.9506 - val_acc: 0.4519\n",
      "Epoch 137/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1350 - acc: 0.9559 - val_loss: 2.3863 - val_acc: 0.5151\n",
      "Epoch 138/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1322 - acc: 0.9554 - val_loss: 3.2263 - val_acc: 0.4257\n",
      "Epoch 139/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1135 - acc: 0.9629 - val_loss: 3.7148 - val_acc: 0.5465\n",
      "Epoch 140/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1790 - acc: 0.9381 - val_loss: 3.8258 - val_acc: 0.4309\n",
      "Epoch 141/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1579 - acc: 0.9479 - val_loss: 1.7888 - val_acc: 0.5330\n",
      "Epoch 142/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1145 - acc: 0.9624 - val_loss: 4.2874 - val_acc: 0.5203\n",
      "Epoch 143/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1171 - acc: 0.9610 - val_loss: 2.3627 - val_acc: 0.5711\n",
      "Epoch 144/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0997 - acc: 0.9672 - val_loss: 2.5730 - val_acc: 0.5107\n",
      "Epoch 145/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1183 - acc: 0.9590 - val_loss: 2.2700 - val_acc: 0.5413\n",
      "Epoch 146/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1045 - acc: 0.9644 - val_loss: 2.2482 - val_acc: 0.5346\n",
      "Epoch 147/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1659 - acc: 0.9461 - val_loss: 2.4203 - val_acc: 0.5647\n",
      "Epoch 148/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1302 - acc: 0.9570 - val_loss: 2.7298 - val_acc: 0.5413\n",
      "Epoch 149/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1174 - acc: 0.9613 - val_loss: 2.3428 - val_acc: 0.5449\n",
      "Epoch 150/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1200 - acc: 0.9623 - val_loss: 6.3107 - val_acc: 0.2518\n",
      "Epoch 151/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0963 - acc: 0.9691 - val_loss: 2.2967 - val_acc: 0.5409\n",
      "Epoch 152/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1226 - acc: 0.9597 - val_loss: 2.5687 - val_acc: 0.5103\n",
      "Epoch 153/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1242 - acc: 0.9572 - val_loss: 2.4400 - val_acc: 0.4980\n",
      "Epoch 154/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1148 - acc: 0.9615 - val_loss: 2.5714 - val_acc: 0.4944\n",
      "Epoch 155/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1206 - acc: 0.9626 - val_loss: 2.3294 - val_acc: 0.5536\n",
      "Epoch 156/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1135 - acc: 0.9652 - val_loss: 2.1976 - val_acc: 0.4909\n",
      "Epoch 157/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1287 - acc: 0.9579 - val_loss: 2.1088 - val_acc: 0.5834\n",
      "Epoch 158/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1025 - acc: 0.9678 - val_loss: 7.9978 - val_acc: 0.3824\n",
      "Epoch 159/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1071 - acc: 0.9636 - val_loss: 2.2196 - val_acc: 0.5814\n",
      "Epoch 160/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0959 - acc: 0.9665 - val_loss: 2.5323 - val_acc: 0.4805\n",
      "Epoch 161/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0799 - acc: 0.9747 - val_loss: 2.6660 - val_acc: 0.4658\n",
      "Epoch 162/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0840 - acc: 0.9724 - val_loss: 4.0634 - val_acc: 0.5171\n",
      "Epoch 163/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1364 - acc: 0.9569 - val_loss: 2.2774 - val_acc: 0.5095\n",
      "Epoch 164/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1073 - acc: 0.9642 - val_loss: 2.4206 - val_acc: 0.5203\n",
      "Epoch 165/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0988 - acc: 0.9677 - val_loss: 2.5463 - val_acc: 0.5913\n",
      "Epoch 166/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0985 - acc: 0.9673 - val_loss: 2.7037 - val_acc: 0.5770\n",
      "Epoch 167/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1007 - acc: 0.9657 - val_loss: 3.6702 - val_acc: 0.5401\n",
      "Epoch 168/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0911 - acc: 0.9703 - val_loss: 2.3281 - val_acc: 0.5230\n",
      "Epoch 169/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0822 - acc: 0.9742 - val_loss: 2.5364 - val_acc: 0.4670\n",
      "Epoch 170/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1878 - acc: 0.9373 - val_loss: 2.2172 - val_acc: 0.5695\n",
      "Epoch 171/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1478 - acc: 0.9479 - val_loss: 3.2155 - val_acc: 0.4472\n",
      "Epoch 172/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1116 - acc: 0.9637 - val_loss: 2.3757 - val_acc: 0.5405\n",
      "Epoch 173/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0934 - acc: 0.9703 - val_loss: 2.5098 - val_acc: 0.5576\n",
      "Epoch 174/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0900 - acc: 0.9695 - val_loss: 2.4852 - val_acc: 0.4976\n",
      "Epoch 175/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0907 - acc: 0.9706 - val_loss: 2.5142 - val_acc: 0.5707\n",
      "Epoch 176/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1165 - acc: 0.9619 - val_loss: 10.2994 - val_acc: 0.2434\n",
      "Epoch 177/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1231 - acc: 0.9603 - val_loss: 2.1829 - val_acc: 0.5433\n",
      "Epoch 178/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0809 - acc: 0.9760 - val_loss: 2.2070 - val_acc: 0.5572\n",
      "Epoch 179/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0819 - acc: 0.9730 - val_loss: 2.3497 - val_acc: 0.5210\n",
      "Epoch 180/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0856 - acc: 0.9719 - val_loss: 2.6857 - val_acc: 0.5052\n",
      "Epoch 181/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0772 - acc: 0.9737 - val_loss: 2.4113 - val_acc: 0.5342\n",
      "Epoch 182/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0891 - acc: 0.9724 - val_loss: 2.4376 - val_acc: 0.5187\n",
      "Epoch 183/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1024 - acc: 0.9662 - val_loss: 2.4637 - val_acc: 0.5274\n",
      "Epoch 184/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1344 - acc: 0.9572 - val_loss: 4.1846 - val_acc: 0.5147\n",
      "Epoch 185/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3885 - acc: 0.8783 - val_loss: 2.4537 - val_acc: 0.5175\n",
      "Epoch 186/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2939 - acc: 0.8981 - val_loss: 6.9741 - val_acc: 0.2446\n",
      "Epoch 187/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3445 - acc: 0.8824 - val_loss: 2.2307 - val_acc: 0.5306\n",
      "Epoch 188/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1702 - acc: 0.9425 - val_loss: 2.0861 - val_acc: 0.5461\n",
      "Epoch 189/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1037 - acc: 0.9685 - val_loss: 1.8107 - val_acc: 0.5894\n",
      "Epoch 190/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0954 - acc: 0.9680 - val_loss: 1.7788 - val_acc: 0.5997\n",
      "Epoch 191/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1012 - acc: 0.9655 - val_loss: 2.2359 - val_acc: 0.5274\n",
      "Epoch 192/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0844 - acc: 0.9735 - val_loss: 2.2187 - val_acc: 0.5508\n",
      "Epoch 193/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0735 - acc: 0.9770 - val_loss: 2.0234 - val_acc: 0.5818\n",
      "Epoch 194/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0839 - acc: 0.9714 - val_loss: 2.4368 - val_acc: 0.5846\n",
      "Epoch 195/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0730 - acc: 0.9771 - val_loss: 1.8772 - val_acc: 0.6052\n",
      "Epoch 196/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0701 - acc: 0.9776 - val_loss: 2.3007 - val_acc: 0.5612\n",
      "Epoch 197/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1038 - acc: 0.9649 - val_loss: 1.9295 - val_acc: 0.6017\n",
      "Epoch 198/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0810 - acc: 0.9732 - val_loss: 2.2064 - val_acc: 0.5902\n",
      "Epoch 199/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0642 - acc: 0.9796 - val_loss: 2.1634 - val_acc: 0.5421\n",
      "Epoch 200/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0775 - acc: 0.9747 - val_loss: 2.0837 - val_acc: 0.5473\n",
      "2019-04-05 17:01:06.730756\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "batch_size =  16 #multiple of 1290/5 (recording length)\n",
    "nb_classes = 6\n",
    "rows, cols = 431, 128  \n",
    "\n",
    "trainlist,validationlist,testlist=fileLists()\n",
    "print('lists sorted')\n",
    "X_tr, Y_train = getData(trainlist)\n",
    "X_v, Y_val = getData(validationlist)\n",
    "X_te, Y_test = getData(testlist)\n",
    "print('data obtained')\n",
    "X_train=np.expand_dims(X_tr,axis=3)\n",
    "X_val=np.expand_dims(X_v,axis=3)\n",
    "X_test=np.expand_dims(X_te,axis=3)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print('data sorted')\n",
    "model = compileCRNN(cols,rows,nb_classes=nb_classes)\n",
    "print('model compiled')\n",
    "savemodelfilename='models\\\\final_model_simple.testsave'\n",
    "samples=(X_train.shape[0])\n",
    "print(datetime.now())\n",
    "models,histories= buildModel(savemodelfilename, samples,model,X_train,Y_train,X_val, Y_val)\n",
    "print(datetime.now())\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
