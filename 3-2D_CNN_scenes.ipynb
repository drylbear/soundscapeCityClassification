{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D,Dropout, Reshape, LSTM, BatchNormalization,TimeDistributed\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam #, RMSprop, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from time import time\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  16 #multiple of 1290/5 (recording length)\n",
    "nb_classes = 10\n",
    "rows, cols = 431, 128   \n",
    "nb_epoch = 200\n",
    "pool_size = (5,5)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.3                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.3              # drop probability for dropout @ fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data sorted\n"
     ]
    }
   ],
   "source": [
    "#might need to rewrite these\n",
    "\n",
    "\n",
    "\n",
    "def fileLists():\n",
    "    trainlist=[]\n",
    "    validationlist=[]\n",
    "    testlist=[]\n",
    "    evalSetupFiles='..\\\\..\\\\CASAdatasets\\\\DCASE18_ASCT1\\\\TUT-urban-acoustic-scenes-2018-development\\\\evaluation_setup\\\\*.txt'\n",
    "    txtfilelist=glob.glob(evalSetupFiles)\n",
    "    for txt in txtfilelist:\n",
    "        if '_location' not in txt:\n",
    "            continue\n",
    "        with open(txt,'r') as evaltxtfile:\n",
    "            for line in evaltxtfile.readlines():\n",
    "                line=line.strip().split('\\t')[0]\n",
    "                if 'train' in txt:\n",
    "                    trainlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "                elif 'test' in txt:\n",
    "                    testlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "                else:\n",
    "                    validationlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "    print('trainfiles: ', str(len(trainlist)))\n",
    "    print('validationfiles: ', str(len(validationlist)))\n",
    "    print('testfiles: ', str(len(testlist)))\n",
    "    return trainlist,validationlist,testlist\n",
    "\n",
    "labelRef={'airport':0,'shopping_mall':1,'metro_station':2,'street_pedestrian':3,'public_square':4,'street_traffic':5,'tram':6,'bus':7,'metro':8,'park':9}\n",
    "\n",
    "def getData(flist):\n",
    "    pth='..\\\\..\\\\CASAdatasets\\\\DCASE18_ASCT1\\\\TUT-urban-acoustic-scenes-2018-development\\\\'\n",
    "    X_=np.zeros(((len(flist)),128,431))\n",
    "    Y_=np.zeros(((len(flist)),nb_classes))\n",
    "    for i,tfile in enumerate(flist):\n",
    "        with open(pth+tfile, \"rb\" ) as scenesample:\n",
    "            fv=pickle.load(scenesample)\n",
    "        X_[i,:,:]=fv\n",
    "        location=tfile.split('-')[0].replace('logMelSpec\\\\','')\n",
    "        Y_[i,labelRef[location]]=1\n",
    "    return X_, Y_\n",
    "#trainlist,validationlist,testlist=fileLists()\n",
    "#X_train, Y_train = getData(trainlist)\n",
    "#X_val, Y_val = getData(validationlist)\n",
    "#X_test, Y_test = getData(testlist)\n",
    "\n",
    "print(' data sorted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 431, 32)      1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 216, 64)       100416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 216, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 108, 128)      32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 54, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 110592)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                7077952   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 7,214,666\n",
      "Trainable params: 7,214,090\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###### Convolutional model\n",
    "def compileCRNN(cols,rows,nb_classes=1):\n",
    "    model = Sequential()\n",
    "    # conv1 layer\n",
    "    model.add(Conv2D(32, (7, 7), padding='same', activation='relu', input_shape=(cols,rows,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv2 layer\n",
    "    model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(4,7), strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv3 layer\n",
    "    model.add(Conv2D(128, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # fc1 layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(prob_drop_hidden))\n",
    "    model.add(BatchNormalization())\n",
    "   \n",
    "    # fc2 layer\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = compileCRNN(cols,rows,nb_classes=nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "#fold=1\n",
    "def train_network(model, model_name, X_train, Y_train, X_val, Y_val, nb_epoch, validationsplit_size, batchsize, early_stoping_patience, output_folder):\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder,model_name + '.hdf5'),save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stoping_patience)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(time()))\n",
    "    Callbacks=[checkpointer,  tensorboard] #early_stopping,\n",
    "  #  print(samples)\n",
    "    steps=int(samples/batchsize)\n",
    "    validationsteps=int(validationsplit_size/batchsize)\n",
    "    history = model.fit(X_train, Y_train, epochs=nb_epoch, callbacks=Callbacks, batch_size=batch_size, validation_data=(X_val, Y_val), shuffle=True, verbose=1)\n",
    "    return history,model\n",
    "\n",
    "def buildModel(savemodelfilename, samples,model,X_train, Y_train,X_val, Y_val):\n",
    "    valSplit_size = int(samples/4)\n",
    "    early_stoping_patience=5\n",
    "    history,model = train_network(model, 'models\\\\sceneModel200_simple', X_train, Y_train, X_val, Y_val, nb_epoch, valSplit_size, batch_size, early_stoping_patience,'.')\n",
    "    model.save_weights(savemodelfilename)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainfiles:  6122\n",
      "validationfiles:  2518\n",
      "testfiles:  2518\n",
      "lists sorted\n",
      "data obtained\n",
      "(6122, 128, 431, 1) (2518, 128, 431, 1) (2518, 128, 431, 1)\n",
      "data sorted\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 128, 431, 32)      1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 216, 64)       100416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 216, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 108, 128)      32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 54, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 110592)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                7077952   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 7,214,666\n",
      "Trainable params: 7,214,090\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "model compiled\n",
      "2019-04-08 17:50:11.055275\n",
      "Train on 6122 samples, validate on 2518 samples\n",
      "Epoch 1/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 2.2504 - acc: 0.1916 - val_loss: 2.1984 - val_acc: 0.1656\n",
      "Epoch 2/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 2.0624 - acc: 0.2524 - val_loss: 2.2344 - val_acc: 0.1998\n",
      "Epoch 3/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.9849 - acc: 0.2927 - val_loss: 2.1485 - val_acc: 0.1930\n",
      "Epoch 4/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.9610 - acc: 0.2935 - val_loss: 2.1220 - val_acc: 0.2438\n",
      "Epoch 5/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.9698 - acc: 0.2979 - val_loss: 3.1195 - val_acc: 0.2311\n",
      "Epoch 6/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.8455 - acc: 0.3483 - val_loss: 10.1634 - val_acc: 0.1898\n",
      "Epoch 7/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.7810 - acc: 0.3780 - val_loss: 6.5154 - val_acc: 0.2637\n",
      "Epoch 8/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.7760 - acc: 0.3757 - val_loss: 2.0900 - val_acc: 0.1783\n",
      "Epoch 9/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.7348 - acc: 0.3861 - val_loss: 1.8786 - val_acc: 0.2371\n",
      "Epoch 10/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.7169 - acc: 0.4051 - val_loss: 9.0974 - val_acc: 0.2236\n",
      "Epoch 11/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.6424 - acc: 0.4234 - val_loss: 7.9958 - val_acc: 0.2474\n",
      "Epoch 12/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.6258 - acc: 0.4278 - val_loss: 5.7271 - val_acc: 0.2427\n",
      "Epoch 13/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.6193 - acc: 0.4183 - val_loss: 1.9781 - val_acc: 0.3546\n",
      "Epoch 14/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.6333 - acc: 0.4222 - val_loss: 2.5490 - val_acc: 0.3487\n",
      "Epoch 15/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.6212 - acc: 0.4074 - val_loss: 4.3532 - val_acc: 0.2661\n",
      "Epoch 16/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.5502 - acc: 0.4396 - val_loss: 6.8551 - val_acc: 0.3070\n",
      "Epoch 17/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4648 - acc: 0.4632 - val_loss: 5.3815 - val_acc: 0.3848\n",
      "Epoch 18/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3981 - acc: 0.4855 - val_loss: 3.0650 - val_acc: 0.4329\n",
      "Epoch 19/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4117 - acc: 0.4838 - val_loss: 7.2332 - val_acc: 0.2959\n",
      "Epoch 20/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4784 - acc: 0.4490 - val_loss: 6.3873 - val_acc: 0.3261\n",
      "Epoch 21/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4565 - acc: 0.4668 - val_loss: 2.4559 - val_acc: 0.3058\n",
      "Epoch 22/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4953 - acc: 0.4616 - val_loss: 2.6153 - val_acc: 0.3733\n",
      "Epoch 23/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4714 - acc: 0.4556 - val_loss: 3.1211 - val_acc: 0.2820\n",
      "Epoch 24/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4185 - acc: 0.4703 - val_loss: 2.7570 - val_acc: 0.3650\n",
      "Epoch 25/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3916 - acc: 0.4905 - val_loss: 2.7873 - val_acc: 0.2347\n",
      "Epoch 26/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4006 - acc: 0.4789 - val_loss: 12.6526 - val_acc: 0.1811\n",
      "Epoch 27/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3599 - acc: 0.4980 - val_loss: 1.7171 - val_acc: 0.3352\n",
      "Epoch 28/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3303 - acc: 0.4997 - val_loss: 3.6902 - val_acc: 0.4063\n",
      "Epoch 29/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3381 - acc: 0.5065 - val_loss: 1.7872 - val_acc: 0.3884\n",
      "Epoch 30/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.2740 - acc: 0.5325 - val_loss: 7.3968 - val_acc: 0.3308\n",
      "Epoch 31/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.2808 - acc: 0.5317 - val_loss: 5.4435 - val_acc: 0.2260\n",
      "Epoch 32/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.2116 - acc: 0.5515 - val_loss: 1.6618 - val_acc: 0.4571\n",
      "Epoch 33/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.1908 - acc: 0.5631 - val_loss: 2.7275 - val_acc: 0.4150\n",
      "Epoch 34/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.4739 - acc: 0.4574 - val_loss: 3.7764 - val_acc: 0.1934\n",
      "Epoch 35/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3661 - acc: 0.4984 - val_loss: 7.2343 - val_acc: 0.2240\n",
      "Epoch 36/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3291 - acc: 0.4997 - val_loss: 8.8369 - val_acc: 0.2641\n",
      "Epoch 37/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.2746 - acc: 0.5315 - val_loss: 10.9690 - val_acc: 0.2339\n",
      "Epoch 38/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.2821 - acc: 0.5314 - val_loss: 1.7251 - val_acc: 0.3817\n",
      "Epoch 39/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.2655 - acc: 0.5312 - val_loss: 4.9875 - val_acc: 0.3316\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.2150 - acc: 0.5547 - val_loss: 6.8449 - val_acc: 0.2923\n",
      "Epoch 41/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3401 - acc: 0.4917 - val_loss: 3.0230 - val_acc: 0.2192\n",
      "Epoch 42/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3230 - acc: 0.5181 - val_loss: 2.0770 - val_acc: 0.3944\n",
      "Epoch 43/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.3197 - acc: 0.5064 - val_loss: 1.7376 - val_acc: 0.3789\n",
      "Epoch 44/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.1816 - acc: 0.5647 - val_loss: 9.4599 - val_acc: 0.2160\n",
      "Epoch 45/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.1720 - acc: 0.5702 - val_loss: 2.1672 - val_acc: 0.3324\n",
      "Epoch 46/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.1784 - acc: 0.5701 - val_loss: 2.1629 - val_acc: 0.3467\n",
      "Epoch 47/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.1943 - acc: 0.5536 - val_loss: 7.0111 - val_acc: 0.2101\n",
      "Epoch 48/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.2429 - acc: 0.5330 - val_loss: 2.9598 - val_acc: 0.2963\n",
      "Epoch 49/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.2566 - acc: 0.5336 - val_loss: 1.7535 - val_acc: 0.4245\n",
      "Epoch 50/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.1427 - acc: 0.5848 - val_loss: 2.6602 - val_acc: 0.2156\n",
      "Epoch 51/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0965 - acc: 0.6036 - val_loss: 1.6143 - val_acc: 0.3928\n",
      "Epoch 52/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0832 - acc: 0.6060 - val_loss: 1.6134 - val_acc: 0.5020\n",
      "Epoch 53/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0626 - acc: 0.6230 - val_loss: 1.6915 - val_acc: 0.4079\n",
      "Epoch 54/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0943 - acc: 0.5996 - val_loss: 4.0934 - val_acc: 0.2117\n",
      "Epoch 55/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0070 - acc: 0.6315 - val_loss: 3.0402 - val_acc: 0.4023\n",
      "Epoch 56/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0379 - acc: 0.6143 - val_loss: 3.0571 - val_acc: 0.2772\n",
      "Epoch 57/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0960 - acc: 0.5983 - val_loss: 1.5835 - val_acc: 0.4841\n",
      "Epoch 58/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.0286 - acc: 0.6225 - val_loss: 1.3989 - val_acc: 0.5302\n",
      "Epoch 59/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9598 - acc: 0.6498 - val_loss: 1.4439 - val_acc: 0.5008\n",
      "Epoch 60/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9546 - acc: 0.6550 - val_loss: 2.4068 - val_acc: 0.3384\n",
      "Epoch 61/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9456 - acc: 0.6625 - val_loss: 2.8537 - val_acc: 0.4047\n",
      "Epoch 62/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9157 - acc: 0.6674 - val_loss: 3.2938 - val_acc: 0.3511\n",
      "Epoch 63/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9180 - acc: 0.6642 - val_loss: 2.2609 - val_acc: 0.4682\n",
      "Epoch 64/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9791 - acc: 0.6475 - val_loss: 1.9169 - val_acc: 0.3963\n",
      "Epoch 65/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9285 - acc: 0.6666 - val_loss: 1.8437 - val_acc: 0.4865\n",
      "Epoch 66/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8832 - acc: 0.6872 - val_loss: 2.3565 - val_acc: 0.3372\n",
      "Epoch 67/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.8969 - acc: 0.6789 - val_loss: 2.0526 - val_acc: 0.3419\n",
      "Epoch 68/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.8982 - acc: 0.6784 - val_loss: 2.0219 - val_acc: 0.3451\n",
      "Epoch 69/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9803 - acc: 0.6488 - val_loss: 2.4020 - val_acc: 0.3145\n",
      "Epoch 70/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0523 - acc: 0.6238 - val_loss: 1.8095 - val_acc: 0.3916\n",
      "Epoch 71/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0028 - acc: 0.6344 - val_loss: 3.7373 - val_acc: 0.3435\n",
      "Epoch 72/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.0085 - acc: 0.6317 - val_loss: 1.7664 - val_acc: 0.4408\n",
      "Epoch 73/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9191 - acc: 0.6664 - val_loss: 1.5954 - val_acc: 0.4682\n",
      "Epoch 74/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9098 - acc: 0.6705 - val_loss: 5.3134 - val_acc: 0.2522\n",
      "Epoch 75/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.8528 - acc: 0.6981 - val_loss: 4.6200 - val_acc: 0.3535\n",
      "Epoch 76/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.8986 - acc: 0.6843 - val_loss: 1.5701 - val_acc: 0.4996\n",
      "Epoch 77/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8314 - acc: 0.7073 - val_loss: 2.7606 - val_acc: 0.4504\n",
      "Epoch 78/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7830 - acc: 0.7228 - val_loss: 2.0372 - val_acc: 0.4488\n",
      "Epoch 79/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7862 - acc: 0.7261 - val_loss: 2.1217 - val_acc: 0.4130\n",
      "Epoch 80/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7701 - acc: 0.7318 - val_loss: 1.7988 - val_acc: 0.4472\n",
      "Epoch 81/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7754 - acc: 0.7259 - val_loss: 3.5977 - val_acc: 0.3630\n",
      "Epoch 82/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7863 - acc: 0.7238 - val_loss: 2.0742 - val_acc: 0.4067\n",
      "Epoch 83/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7689 - acc: 0.7266 - val_loss: 1.7126 - val_acc: 0.5044\n",
      "Epoch 84/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7409 - acc: 0.7331 - val_loss: 1.4936 - val_acc: 0.5083\n",
      "Epoch 85/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7403 - acc: 0.7453 - val_loss: 1.7007 - val_acc: 0.3856\n",
      "Epoch 86/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6845 - acc: 0.7602 - val_loss: 1.8533 - val_acc: 0.4492\n",
      "Epoch 87/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.8273 - acc: 0.7012 - val_loss: 1.4743 - val_acc: 0.4972\n",
      "Epoch 88/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7834 - acc: 0.7177 - val_loss: 3.4491 - val_acc: 0.3118\n",
      "Epoch 89/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7728 - acc: 0.7225 - val_loss: 1.3458 - val_acc: 0.5032\n",
      "Epoch 90/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6925 - acc: 0.7565 - val_loss: 8.1804 - val_acc: 0.1831\n",
      "Epoch 91/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7122 - acc: 0.7445 - val_loss: 1.5763 - val_acc: 0.5397\n",
      "Epoch 92/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6817 - acc: 0.7599 - val_loss: 1.3881 - val_acc: 0.5353\n",
      "Epoch 93/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6488 - acc: 0.7733 - val_loss: 1.1582 - val_acc: 0.5993\n",
      "Epoch 94/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7735 - acc: 0.7249 - val_loss: 3.6350 - val_acc: 0.4361\n",
      "Epoch 95/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7019 - acc: 0.7512 - val_loss: 1.6762 - val_acc: 0.5242\n",
      "Epoch 96/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6691 - acc: 0.7569 - val_loss: 2.0504 - val_acc: 0.4936\n",
      "Epoch 97/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6569 - acc: 0.7739 - val_loss: 1.7865 - val_acc: 0.4114\n",
      "Epoch 98/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7388 - acc: 0.7393 - val_loss: 3.3715 - val_acc: 0.3090\n",
      "Epoch 99/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.8000 - acc: 0.7123 - val_loss: 1.6819 - val_acc: 0.4833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7052 - acc: 0.7468 - val_loss: 2.6544 - val_acc: 0.2351\n",
      "Epoch 101/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7230 - acc: 0.7434 - val_loss: 5.0746 - val_acc: 0.3884\n",
      "Epoch 102/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6936 - acc: 0.7576 - val_loss: 4.4962 - val_acc: 0.3268\n",
      "Epoch 103/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6504 - acc: 0.7679 - val_loss: 1.8164 - val_acc: 0.4897\n",
      "Epoch 104/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7768 - acc: 0.7225 - val_loss: 1.6578 - val_acc: 0.4921\n",
      "Epoch 105/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6905 - acc: 0.7558 - val_loss: 1.7338 - val_acc: 0.5099\n",
      "Epoch 106/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6547 - acc: 0.7694 - val_loss: 2.8250 - val_acc: 0.3185\n",
      "Epoch 107/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7238 - acc: 0.7367 - val_loss: 1.6259 - val_acc: 0.4210\n",
      "Epoch 108/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6482 - acc: 0.7700 - val_loss: 2.3955 - val_acc: 0.4678\n",
      "Epoch 109/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5948 - acc: 0.7981 - val_loss: 1.4160 - val_acc: 0.5739\n",
      "Epoch 110/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5982 - acc: 0.7901 - val_loss: 1.7230 - val_acc: 0.4476\n",
      "Epoch 111/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7336 - acc: 0.7429 - val_loss: 1.5461 - val_acc: 0.5572\n",
      "Epoch 112/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9938 - acc: 0.6415 - val_loss: 2.1403 - val_acc: 0.3542\n",
      "Epoch 113/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.9664 - acc: 0.6519 - val_loss: 2.6490 - val_acc: 0.4742\n",
      "Epoch 114/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7673 - acc: 0.7197 - val_loss: 1.8862 - val_acc: 0.5254\n",
      "Epoch 115/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7550 - acc: 0.7337 - val_loss: 1.5285 - val_acc: 0.4897\n",
      "Epoch 116/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6022 - acc: 0.7885 - val_loss: 1.3574 - val_acc: 0.5433\n",
      "Epoch 117/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5936 - acc: 0.7914 - val_loss: 2.7725 - val_acc: 0.2295\n",
      "Epoch 118/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5627 - acc: 0.8055 - val_loss: 1.5549 - val_acc: 0.5199\n",
      "Epoch 119/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5446 - acc: 0.8143 - val_loss: 1.2924 - val_acc: 0.5909\n",
      "Epoch 120/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5307 - acc: 0.8159 - val_loss: 3.0610 - val_acc: 0.4790\n",
      "Epoch 121/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5427 - acc: 0.8146 - val_loss: 1.3690 - val_acc: 0.5230\n",
      "Epoch 122/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6361 - acc: 0.7796 - val_loss: 2.9665 - val_acc: 0.3773\n",
      "Epoch 123/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6872 - acc: 0.7592 - val_loss: 1.4858 - val_acc: 0.5624\n",
      "Epoch 124/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5741 - acc: 0.7975 - val_loss: 1.7756 - val_acc: 0.4273\n",
      "Epoch 125/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6306 - acc: 0.7803 - val_loss: 1.8996 - val_acc: 0.5719\n",
      "Epoch 126/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5780 - acc: 0.7940 - val_loss: 2.0460 - val_acc: 0.4980\n",
      "Epoch 127/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5576 - acc: 0.8038 - val_loss: 1.5082 - val_acc: 0.5826\n",
      "Epoch 128/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6214 - acc: 0.7904 - val_loss: 14.4404 - val_acc: 0.1041\n",
      "Epoch 129/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7329 - acc: 0.7427 - val_loss: 1.9580 - val_acc: 0.4766\n",
      "Epoch 130/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7235 - acc: 0.7439 - val_loss: 3.0553 - val_acc: 0.3713\n",
      "Epoch 131/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6661 - acc: 0.7679 - val_loss: 1.7209 - val_acc: 0.5103\n",
      "Epoch 132/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5974 - acc: 0.7929 - val_loss: 5.1649 - val_acc: 0.4142\n",
      "Epoch 133/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6386 - acc: 0.7793 - val_loss: 3.5405 - val_acc: 0.4464\n",
      "Epoch 134/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5744 - acc: 0.7976 - val_loss: 1.3602 - val_acc: 0.5719\n",
      "Epoch 135/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5600 - acc: 0.8045 - val_loss: 1.4831 - val_acc: 0.5584\n",
      "Epoch 136/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5324 - acc: 0.8192 - val_loss: 1.4483 - val_acc: 0.4972\n",
      "Epoch 137/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5572 - acc: 0.8102 - val_loss: 1.6369 - val_acc: 0.5310\n",
      "Epoch 138/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6729 - acc: 0.7710 - val_loss: 1.5698 - val_acc: 0.4595\n",
      "Epoch 139/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6614 - acc: 0.7651 - val_loss: 3.9667 - val_acc: 0.3602\n",
      "Epoch 140/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.8034 - acc: 0.7166 - val_loss: 5.8172 - val_acc: 0.2986\n",
      "Epoch 141/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7443 - acc: 0.7391 - val_loss: 2.0068 - val_acc: 0.4635\n",
      "Epoch 142/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7646 - acc: 0.7339 - val_loss: 4.9837 - val_acc: 0.1942\n",
      "Epoch 143/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7590 - acc: 0.7337 - val_loss: 1.5206 - val_acc: 0.4790\n",
      "Epoch 144/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7410 - acc: 0.7349 - val_loss: 1.6838 - val_acc: 0.4996\n",
      "Epoch 145/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.7310 - acc: 0.7426 - val_loss: 1.8799 - val_acc: 0.4658\n",
      "Epoch 146/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6455 - acc: 0.7692 - val_loss: 2.6853 - val_acc: 0.5314\n",
      "Epoch 147/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5846 - acc: 0.7953 - val_loss: 1.3199 - val_acc: 0.6009\n",
      "Epoch 148/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 1.1202 - acc: 0.6256 - val_loss: 7.6352 - val_acc: 0.3153\n",
      "Epoch 149/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6976 - acc: 0.7658 - val_loss: 3.2277 - val_acc: 0.4055\n",
      "Epoch 150/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.8224 - acc: 0.7215 - val_loss: 2.1637 - val_acc: 0.4611\n",
      "Epoch 151/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6926 - acc: 0.7563 - val_loss: 1.5151 - val_acc: 0.5353\n",
      "Epoch 152/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.6127 - acc: 0.7862 - val_loss: 1.6628 - val_acc: 0.4960\n",
      "Epoch 153/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5846 - acc: 0.7926 - val_loss: 1.3553 - val_acc: 0.5596\n",
      "Epoch 154/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5121 - acc: 0.8187 - val_loss: 1.5911 - val_acc: 0.5695\n",
      "Epoch 155/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5098 - acc: 0.8244 - val_loss: 1.8447 - val_acc: 0.5282\n",
      "Epoch 156/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4827 - acc: 0.8324 - val_loss: 1.9988 - val_acc: 0.4480\n",
      "Epoch 157/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4744 - acc: 0.8350 - val_loss: 1.7742 - val_acc: 0.5492\n",
      "Epoch 158/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4390 - acc: 0.8525 - val_loss: 1.3040 - val_acc: 0.5719\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4506 - acc: 0.8412 - val_loss: 1.3676 - val_acc: 0.5635\n",
      "Epoch 160/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4096 - acc: 0.8571 - val_loss: 1.2141 - val_acc: 0.6140\n",
      "Epoch 161/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4458 - acc: 0.8492 - val_loss: 2.1990 - val_acc: 0.4098\n",
      "Epoch 162/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4993 - acc: 0.8293 - val_loss: 1.4843 - val_acc: 0.5258\n",
      "Epoch 163/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4793 - acc: 0.8350 - val_loss: 1.4751 - val_acc: 0.5556\n",
      "Epoch 164/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4247 - acc: 0.8623 - val_loss: 3.7097 - val_acc: 0.2685\n",
      "Epoch 165/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4219 - acc: 0.8499 - val_loss: 1.8342 - val_acc: 0.5250\n",
      "Epoch 166/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3761 - acc: 0.8736 - val_loss: 1.4776 - val_acc: 0.5655\n",
      "Epoch 167/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4341 - acc: 0.8451 - val_loss: 1.5877 - val_acc: 0.5290\n",
      "Epoch 168/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4352 - acc: 0.8497 - val_loss: 2.0688 - val_acc: 0.4269\n",
      "Epoch 169/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4208 - acc: 0.8551 - val_loss: 1.3727 - val_acc: 0.5794\n",
      "Epoch 170/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3714 - acc: 0.8798 - val_loss: 1.8261 - val_acc: 0.5207\n",
      "Epoch 171/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5901 - acc: 0.7908 - val_loss: 2.1156 - val_acc: 0.3868\n",
      "Epoch 172/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4734 - acc: 0.8324 - val_loss: 2.1180 - val_acc: 0.4968\n",
      "Epoch 173/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4045 - acc: 0.8595 - val_loss: 1.3535 - val_acc: 0.5631\n",
      "Epoch 174/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4720 - acc: 0.8304 - val_loss: 5.0282 - val_acc: 0.4182\n",
      "Epoch 175/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4387 - acc: 0.8461 - val_loss: 12.8739 - val_acc: 0.1521\n",
      "Epoch 176/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5609 - acc: 0.7989 - val_loss: 1.3727 - val_acc: 0.5592\n",
      "Epoch 177/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4860 - acc: 0.8241 - val_loss: 3.4491 - val_acc: 0.2510\n",
      "Epoch 178/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4634 - acc: 0.8391 - val_loss: 1.5373 - val_acc: 0.5524\n",
      "Epoch 179/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4321 - acc: 0.8445 - val_loss: 8.3581 - val_acc: 0.3137\n",
      "Epoch 180/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4573 - acc: 0.8434 - val_loss: 1.8738 - val_acc: 0.5151\n",
      "Epoch 181/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5078 - acc: 0.8206 - val_loss: 2.1328 - val_acc: 0.4496\n",
      "Epoch 182/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4183 - acc: 0.8548 - val_loss: 1.4366 - val_acc: 0.5719\n",
      "Epoch 183/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4044 - acc: 0.8631 - val_loss: 1.3094 - val_acc: 0.6052\n",
      "Epoch 184/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4698 - acc: 0.8314 - val_loss: 1.5876 - val_acc: 0.5624\n",
      "Epoch 185/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5585 - acc: 0.8025 - val_loss: 1.8663 - val_acc: 0.3662\n",
      "Epoch 186/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5463 - acc: 0.8048 - val_loss: 1.4565 - val_acc: 0.5496\n",
      "Epoch 187/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4590 - acc: 0.8411 - val_loss: 1.4818 - val_acc: 0.5655\n",
      "Epoch 188/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4761 - acc: 0.8339 - val_loss: 1.4424 - val_acc: 0.4980\n",
      "Epoch 189/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4358 - acc: 0.8465 - val_loss: 1.5110 - val_acc: 0.5592\n",
      "Epoch 190/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4252 - acc: 0.8483 - val_loss: 2.0709 - val_acc: 0.4456\n",
      "Epoch 191/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3677 - acc: 0.8765 - val_loss: 1.5054 - val_acc: 0.5429\n",
      "Epoch 192/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3668 - acc: 0.8777 - val_loss: 1.2679 - val_acc: 0.5961\n",
      "Epoch 193/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3317 - acc: 0.8924 - val_loss: 1.4634 - val_acc: 0.5842\n",
      "Epoch 194/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3793 - acc: 0.8714 - val_loss: 2.8712 - val_acc: 0.4237\n",
      "Epoch 195/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3806 - acc: 0.8692 - val_loss: 1.4369 - val_acc: 0.5838\n",
      "Epoch 196/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4175 - acc: 0.8553 - val_loss: 1.5562 - val_acc: 0.5441\n",
      "Epoch 197/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4167 - acc: 0.8577 - val_loss: 2.2056 - val_acc: 0.4015\n",
      "Epoch 198/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3751 - acc: 0.8757 - val_loss: 1.5811 - val_acc: 0.5699\n",
      "Epoch 199/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3326 - acc: 0.8875 - val_loss: 1.4974 - val_acc: 0.5818\n",
      "Epoch 200/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3812 - acc: 0.8674 - val_loss: 2.1412 - val_acc: 0.4504\n",
      "2019-04-08 19:08:03.827662\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "batch_size =  16 #multiple of 1290/5 (recording length)\n",
    "nb_classes = 10\n",
    "rows, cols = 431, 128  \n",
    "nb_epoch = 200\n",
    "pool_size = (3,3)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.25                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.5              # drop probability for dropout @ fc layer\n",
    "\n",
    "\n",
    "trainlist,validationlist,testlist=fileLists()\n",
    "print('lists sorted')\n",
    "X_tr, Y_train = getData(trainlist)\n",
    "X_v, Y_val = getData(validationlist)\n",
    "X_te, Y_test = getData(testlist)\n",
    "print('data obtained')\n",
    "X_train=np.expand_dims(X_tr,axis=3)\n",
    "X_val=np.expand_dims(X_v,axis=3)\n",
    "X_test=np.expand_dims(X_te,axis=3)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print('data sorted')\n",
    "model = compileCRNN(cols,rows,nb_classes=nb_classes)\n",
    "#model = load_model('models\\\\sceneModel200.hdf5')\n",
    "print('model compiled')\n",
    "savemodelfilename='models\\\\sceneModel200_simple.testsave'\n",
    "samples=(X_train.shape[0])\n",
    "print(datetime.now())\n",
    "models,histories= buildModel(savemodelfilename, samples,model,X_train,Y_train,X_val, Y_val)\n",
    "print(datetime.now())\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
