{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D,Dropout, Reshape, LSTM, BatchNormalization,TimeDistributed\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam #, RMSprop, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from time import time\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  16 #multiple of 1290/5 (recording length)\n",
    "nb_classes = 3\n",
    "rows, cols = 431, 128   \n",
    "nb_epoch = 200\n",
    "pool_size = (5,5)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.3                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.3              # drop probability for dropout @ fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data sorted\n"
     ]
    }
   ],
   "source": [
    "#might need to rewrite these\n",
    "\n",
    "def fileLists():\n",
    "    trainlist=[]\n",
    "    validationlist=[]\n",
    "    testlist=[]\n",
    "    evalSetupFiles='..\\\\..\\\\CASAdatasets\\\\DCASE18_ASCT1\\\\TUT-urban-acoustic-scenes-2018-development\\\\evaluation_setup\\\\*.txt'\n",
    "    txtfilelist=glob.glob(evalSetupFiles)\n",
    "    for txt in txtfilelist:\n",
    "        if '_location' not in txt:\n",
    "            continue\n",
    "        with open(txt,'r') as evaltxtfile:\n",
    "            for line in evaltxtfile.readlines():\n",
    "                line=line.strip().split('\\t')[0]\n",
    "                if 'train' in txt:\n",
    "                    trainlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "                elif 'test' in txt:\n",
    "                    testlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "                else:\n",
    "                    validationlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "    print('trainfiles: ', str(len(trainlist)))\n",
    "    print('validationfiles: ', str(len(validationlist)))\n",
    "    print('testfiles: ', str(len(testlist)))\n",
    "    return trainlist,validationlist,testlist\n",
    "\n",
    "labelRef={'airport':0,'shopping_mall':1,'metro_station':2,'street_pedestrian':3,'public_square':4,'street_traffic':5,'tram':6,'bus':7,'metro':8,'park':9}\n",
    "labelRef={'inside':0, 'outside':1, 'transport':2}\n",
    "def getData(flist):\n",
    "    pth='..\\\\..\\\\CASAdatasets\\\\DCASE18_ASCT1\\\\TUT-urban-acoustic-scenes-2018-development\\\\'\n",
    "    X_=np.zeros(((len(flist)),128,431))\n",
    "    Y_=np.zeros(((len(flist)),nb_classes))\n",
    "    for i,tfile in enumerate(flist):\n",
    "        with open(pth+tfile, \"rb\" ) as scenesample:\n",
    "            fv=pickle.load(scenesample)\n",
    "        X_[i,:,:]=fv\n",
    "        location=tfile.split('-')[0].replace('logMelSpec\\\\','')\n",
    "        if location in ['airport','shopping_mall','metro_station']:\n",
    "            location='inside'\n",
    "        elif location in ['street_pedestrian','public_square','street_traffic','park']:\n",
    "            location='outside'\n",
    "        else:\n",
    "            location='transport'\n",
    "        Y_[i,labelRef[location]]=1\n",
    "    return X_, Y_\n",
    "#trainlist,validationlist,testlist=fileLists()\n",
    "#X_train, Y_train = getData(trainlist)\n",
    "#X_val, Y_val = getData(validationlist)\n",
    "#X_test, Y_test = getData(testlist)\n",
    "\n",
    "print(' data sorted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 128, 431, 32)      1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 216, 64)       100416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 216, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 108, 128)      32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 54, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 110592)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                7077952   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 7,214,211\n",
      "Trainable params: 7,213,635\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###### Convolutional model\n",
    "def compileCRNN(cols,rows,nb_classes=1):\n",
    "    model = Sequential()\n",
    "    # conv1 layer\n",
    "    model.add(Conv2D(32, (7, 7), padding='same', activation='relu', input_shape=(cols,rows,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv2 layer\n",
    "    model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(4,7), strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv3 layer\n",
    "    model.add(Conv2D(128, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # fc1 layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(prob_drop_hidden))\n",
    "    model.add(BatchNormalization())\n",
    "   \n",
    "    # fc2 layer\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = compileCRNN(cols,rows,nb_classes=nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "#fold=1\n",
    "def train_network(model, model_name, X_train, Y_train, X_val, Y_val, nb_epoch, validationsplit_size, batchsize, early_stoping_patience, output_folder):\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder,model_name + '.hdf5'),save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stoping_patience)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(time()))\n",
    "    Callbacks=[checkpointer,  tensorboard] #early_stopping,\n",
    "  #  print(samples)\n",
    "    steps=int(samples/batchsize)\n",
    "    validationsteps=int(validationsplit_size/batchsize)\n",
    "    history = model.fit(X_train, Y_train, epochs=nb_epoch, callbacks=Callbacks, batch_size=batch_size, validation_data=(X_val, Y_val), shuffle=True, verbose=1)\n",
    "    return history,model\n",
    "\n",
    "def buildModel(savemodelfilename, samples,model,X_train, Y_train,X_val, Y_val):\n",
    "    valSplit_size = int(samples/4)\n",
    "    early_stoping_patience=5\n",
    "    history,model = train_network(model, 'models\\\\3groupedScenes', X_train, Y_train, X_val, Y_val, nb_epoch, valSplit_size, batch_size, early_stoping_patience,'.')\n",
    "    model.save_weights(savemodelfilename)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainfiles:  6122\n",
      "validationfiles:  2518\n",
      "testfiles:  2518\n",
      "lists sorted\n",
      "data obtained\n",
      "(6122, 128, 431, 1) (2518, 128, 431, 1) (2518, 128, 431, 1)\n",
      "data sorted\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 128, 431, 32)      1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 216, 64)       100416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 216, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 108, 128)      32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 54, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 110592)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                7077952   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 7,214,211\n",
      "Trainable params: 7,213,635\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "model compiled\n",
      "2019-04-09 12:21:45.958215\n",
      "Train on 6122 samples, validate on 2518 samples\n",
      "Epoch 1/200\n",
      "6122/6122 [==============================] - 26s 4ms/step - loss: 0.8343 - acc: 0.6392 - val_loss: 1.1743 - val_acc: 0.6728\n",
      "Epoch 2/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5815 - acc: 0.7702 - val_loss: 3.5589 - val_acc: 0.4515\n",
      "Epoch 3/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.5066 - acc: 0.8027 - val_loss: 3.8584 - val_acc: 0.3376\n",
      "Epoch 4/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4578 - acc: 0.8210 - val_loss: 5.6079 - val_acc: 0.3419\n",
      "Epoch 5/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4565 - acc: 0.8251 - val_loss: 7.0359 - val_acc: 0.4047\n",
      "Epoch 6/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.4182 - acc: 0.8473 - val_loss: 3.6125 - val_acc: 0.5429\n",
      "Epoch 7/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3891 - acc: 0.8510 - val_loss: 5.2531 - val_acc: 0.4913\n",
      "Epoch 8/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3846 - acc: 0.8558 - val_loss: 5.2783 - val_acc: 0.5135\n",
      "Epoch 9/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3666 - acc: 0.8665 - val_loss: 5.5298 - val_acc: 0.4889\n",
      "Epoch 10/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3538 - acc: 0.8682 - val_loss: 1.9940 - val_acc: 0.6795\n",
      "Epoch 11/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3292 - acc: 0.8819 - val_loss: 8.6936 - val_acc: 0.3940\n",
      "Epoch 12/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3283 - acc: 0.8799 - val_loss: 1.8997 - val_acc: 0.5969\n",
      "Epoch 13/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3593 - acc: 0.8641 - val_loss: 7.6393 - val_acc: 0.3817\n",
      "Epoch 14/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3598 - acc: 0.8620 - val_loss: 9.5046 - val_acc: 0.3439\n",
      "Epoch 15/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3490 - acc: 0.8633 - val_loss: 1.6840 - val_acc: 0.5318\n",
      "Epoch 16/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3295 - acc: 0.8777 - val_loss: 7.1905 - val_acc: 0.4098\n",
      "Epoch 17/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2960 - acc: 0.8914 - val_loss: 5.1057 - val_acc: 0.4932\n",
      "Epoch 18/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2805 - acc: 0.8994 - val_loss: 3.2831 - val_acc: 0.6132\n",
      "Epoch 19/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2607 - acc: 0.9017 - val_loss: 7.1401 - val_acc: 0.4543\n",
      "Epoch 20/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3081 - acc: 0.8840 - val_loss: 9.3063 - val_acc: 0.3427\n",
      "Epoch 21/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2970 - acc: 0.8906 - val_loss: 9.0955 - val_acc: 0.3566\n",
      "Epoch 22/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2850 - acc: 0.8963 - val_loss: 2.3893 - val_acc: 0.6450\n",
      "Epoch 23/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2757 - acc: 0.8956 - val_loss: 2.3433 - val_acc: 0.3975\n",
      "Epoch 24/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3289 - acc: 0.8752 - val_loss: 3.7257 - val_acc: 0.5496\n",
      "Epoch 25/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2918 - acc: 0.8896 - val_loss: 9.2784 - val_acc: 0.2851\n",
      "Epoch 26/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3162 - acc: 0.8793 - val_loss: 10.0490 - val_acc: 0.2986\n",
      "Epoch 27/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2605 - acc: 0.9026 - val_loss: 1.8285 - val_acc: 0.6771\n",
      "Epoch 28/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2694 - acc: 0.9025 - val_loss: 5.0121 - val_acc: 0.5520\n",
      "Epoch 29/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2637 - acc: 0.9033 - val_loss: 1.7370 - val_acc: 0.6930\n",
      "Epoch 30/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2508 - acc: 0.9075 - val_loss: 5.8493 - val_acc: 0.3332\n",
      "Epoch 31/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2372 - acc: 0.9151 - val_loss: 2.8153 - val_acc: 0.6724\n",
      "Epoch 32/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2138 - acc: 0.9249 - val_loss: 8.4515 - val_acc: 0.4063\n",
      "Epoch 33/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2302 - acc: 0.9165 - val_loss: 4.7380 - val_acc: 0.6017\n",
      "Epoch 34/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2044 - acc: 0.9254 - val_loss: 10.5019 - val_acc: 0.3300\n",
      "Epoch 35/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2068 - acc: 0.9244 - val_loss: 5.6040 - val_acc: 0.5552\n",
      "Epoch 36/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2068 - acc: 0.9252 - val_loss: 0.4804 - val_acc: 0.8443\n",
      "Epoch 37/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1792 - acc: 0.9384 - val_loss: 7.1593 - val_acc: 0.4293\n",
      "Epoch 38/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1875 - acc: 0.9316 - val_loss: 1.8210 - val_acc: 0.6875\n",
      "Epoch 39/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2845 - acc: 0.8943 - val_loss: 0.7706 - val_acc: 0.7125\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2370 - acc: 0.9162 - val_loss: 2.7808 - val_acc: 0.6120\n",
      "Epoch 41/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2225 - acc: 0.9165 - val_loss: 2.0684 - val_acc: 0.6048\n",
      "Epoch 42/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2628 - acc: 0.9025 - val_loss: 0.6689 - val_acc: 0.8129\n",
      "Epoch 43/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2251 - acc: 0.9182 - val_loss: 2.4042 - val_acc: 0.6926\n",
      "Epoch 44/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1988 - acc: 0.9306 - val_loss: 0.9008 - val_acc: 0.8237\n",
      "Epoch 45/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1827 - acc: 0.9355 - val_loss: 0.5188 - val_acc: 0.8403\n",
      "Epoch 46/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1727 - acc: 0.9414 - val_loss: 0.8069 - val_acc: 0.8471\n",
      "Epoch 47/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3674 - acc: 0.8597 - val_loss: 6.8865 - val_acc: 0.4408\n",
      "Epoch 48/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2916 - acc: 0.8964 - val_loss: 0.6739 - val_acc: 0.8237\n",
      "Epoch 49/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2597 - acc: 0.9056 - val_loss: 1.3744 - val_acc: 0.5898\n",
      "Epoch 50/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2762 - acc: 0.8910 - val_loss: 0.8416 - val_acc: 0.7887\n",
      "Epoch 51/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2132 - acc: 0.9218 - val_loss: 1.0169 - val_acc: 0.7546\n",
      "Epoch 52/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2010 - acc: 0.9237 - val_loss: 0.6223 - val_acc: 0.8292\n",
      "Epoch 53/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1740 - acc: 0.9342 - val_loss: 3.6134 - val_acc: 0.5759\n",
      "Epoch 54/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1577 - acc: 0.9464 - val_loss: 1.3623 - val_acc: 0.7434\n",
      "Epoch 55/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1591 - acc: 0.9440 - val_loss: 0.5681 - val_acc: 0.8443\n",
      "Epoch 56/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1557 - acc: 0.9440 - val_loss: 1.4945 - val_acc: 0.7470\n",
      "Epoch 57/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1740 - acc: 0.9386 - val_loss: 1.0078 - val_acc: 0.7057\n",
      "Epoch 58/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1701 - acc: 0.9381 - val_loss: 1.2231 - val_acc: 0.7673\n",
      "Epoch 59/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1868 - acc: 0.9312 - val_loss: 1.0687 - val_acc: 0.7887\n",
      "Epoch 60/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1562 - acc: 0.9412 - val_loss: 7.2889 - val_acc: 0.3713\n",
      "Epoch 61/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2288 - acc: 0.9167 - val_loss: 0.5890 - val_acc: 0.8713\n",
      "Epoch 62/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1686 - acc: 0.9338 - val_loss: 1.5909 - val_acc: 0.7883\n",
      "Epoch 63/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1544 - acc: 0.9427 - val_loss: 1.4816 - val_acc: 0.7661\n",
      "Epoch 64/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1471 - acc: 0.9472 - val_loss: 2.3281 - val_acc: 0.6708\n",
      "Epoch 65/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1301 - acc: 0.9559 - val_loss: 2.0842 - val_acc: 0.7331\n",
      "Epoch 66/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1349 - acc: 0.9521 - val_loss: 0.6313 - val_acc: 0.8336\n",
      "Epoch 67/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1190 - acc: 0.9598 - val_loss: 0.7713 - val_acc: 0.8237\n",
      "Epoch 68/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1360 - acc: 0.9492 - val_loss: 0.6855 - val_acc: 0.7971\n",
      "Epoch 69/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1820 - acc: 0.9343 - val_loss: 0.9498 - val_acc: 0.8066\n",
      "Epoch 70/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1109 - acc: 0.9601 - val_loss: 0.6006 - val_acc: 0.8360\n",
      "Epoch 71/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1146 - acc: 0.9624 - val_loss: 0.7281 - val_acc: 0.7979\n",
      "Epoch 72/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1011 - acc: 0.9675 - val_loss: 0.4681 - val_acc: 0.8582\n",
      "Epoch 73/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1106 - acc: 0.9610 - val_loss: 0.5503 - val_acc: 0.8491\n",
      "Epoch 74/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1108 - acc: 0.9611 - val_loss: 1.4264 - val_acc: 0.6394\n",
      "Epoch 75/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1309 - acc: 0.9554 - val_loss: 0.4258 - val_acc: 0.8721\n",
      "Epoch 76/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1286 - acc: 0.9556 - val_loss: 1.4965 - val_acc: 0.6875\n",
      "Epoch 77/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1188 - acc: 0.9611 - val_loss: 0.4222 - val_acc: 0.8773\n",
      "Epoch 78/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0979 - acc: 0.9650 - val_loss: 0.6227 - val_acc: 0.8316\n",
      "Epoch 79/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0929 - acc: 0.9690 - val_loss: 1.6868 - val_acc: 0.7502\n",
      "Epoch 80/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1070 - acc: 0.9652 - val_loss: 0.4159 - val_acc: 0.8705\n",
      "Epoch 81/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0985 - acc: 0.9673 - val_loss: 1.3937 - val_acc: 0.7875\n",
      "Epoch 82/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0945 - acc: 0.9668 - val_loss: 0.6801 - val_acc: 0.8435\n",
      "Epoch 83/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1026 - acc: 0.9642 - val_loss: 0.9430 - val_acc: 0.7867\n",
      "Epoch 84/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1333 - acc: 0.9515 - val_loss: 0.5582 - val_acc: 0.8451\n",
      "Epoch 85/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1018 - acc: 0.9647 - val_loss: 1.1215 - val_acc: 0.7943\n",
      "Epoch 86/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1016 - acc: 0.9668 - val_loss: 2.1138 - val_acc: 0.7450\n",
      "Epoch 87/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0853 - acc: 0.9722 - val_loss: 2.3711 - val_acc: 0.7268\n",
      "Epoch 88/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0971 - acc: 0.9678 - val_loss: 2.2857 - val_acc: 0.5723\n",
      "Epoch 89/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0949 - acc: 0.9695 - val_loss: 0.6695 - val_acc: 0.8292\n",
      "Epoch 90/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0789 - acc: 0.9721 - val_loss: 0.7176 - val_acc: 0.7740\n",
      "Epoch 91/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0871 - acc: 0.9714 - val_loss: 0.6801 - val_acc: 0.8288\n",
      "Epoch 92/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0708 - acc: 0.9750 - val_loss: 3.5400 - val_acc: 0.6807\n",
      "Epoch 93/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0783 - acc: 0.9727 - val_loss: 1.6645 - val_acc: 0.7832\n",
      "Epoch 94/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0786 - acc: 0.9735 - val_loss: 1.2916 - val_acc: 0.6612\n",
      "Epoch 95/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0841 - acc: 0.9703 - val_loss: 4.8032 - val_acc: 0.6017\n",
      "Epoch 96/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0707 - acc: 0.9778 - val_loss: 0.7623 - val_acc: 0.8002\n",
      "Epoch 97/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0798 - acc: 0.9737 - val_loss: 1.7037 - val_acc: 0.6863\n",
      "Epoch 98/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0724 - acc: 0.9757 - val_loss: 1.5854 - val_acc: 0.7029\n",
      "Epoch 99/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0694 - acc: 0.9768 - val_loss: 3.6731 - val_acc: 0.6716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0690 - acc: 0.9794 - val_loss: 1.0413 - val_acc: 0.8388\n",
      "Epoch 101/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0718 - acc: 0.9775 - val_loss: 2.3871 - val_acc: 0.6914\n",
      "Epoch 102/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0655 - acc: 0.9771 - val_loss: 0.7451 - val_acc: 0.7836\n",
      "Epoch 103/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0798 - acc: 0.9714 - val_loss: 1.0446 - val_acc: 0.6513\n",
      "Epoch 104/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0864 - acc: 0.9696 - val_loss: 4.1576 - val_acc: 0.6442\n",
      "Epoch 105/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0644 - acc: 0.9786 - val_loss: 0.8506 - val_acc: 0.8102\n",
      "Epoch 106/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0786 - acc: 0.9752 - val_loss: 0.4865 - val_acc: 0.8789\n",
      "Epoch 107/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0765 - acc: 0.9742 - val_loss: 1.1941 - val_acc: 0.7188\n",
      "Epoch 108/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0634 - acc: 0.9802 - val_loss: 1.5477 - val_acc: 0.5492\n",
      "Epoch 109/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0645 - acc: 0.9753 - val_loss: 4.0958 - val_acc: 0.6279\n",
      "Epoch 110/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0728 - acc: 0.9748 - val_loss: 4.2594 - val_acc: 0.6132\n",
      "Epoch 111/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0649 - acc: 0.9791 - val_loss: 1.5480 - val_acc: 0.8046\n",
      "Epoch 112/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0711 - acc: 0.9752 - val_loss: 0.7559 - val_acc: 0.8352\n",
      "Epoch 113/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0650 - acc: 0.9763 - val_loss: 2.5010 - val_acc: 0.7466\n",
      "Epoch 114/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0538 - acc: 0.9827 - val_loss: 2.3249 - val_acc: 0.7260\n",
      "Epoch 115/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0561 - acc: 0.9806 - val_loss: 5.2666 - val_acc: 0.5798\n",
      "Epoch 116/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0789 - acc: 0.9716 - val_loss: 1.3499 - val_acc: 0.7125\n",
      "Epoch 117/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0660 - acc: 0.9778 - val_loss: 0.7282 - val_acc: 0.8400\n",
      "Epoch 118/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0726 - acc: 0.9762 - val_loss: 0.6937 - val_acc: 0.8574\n",
      "Epoch 119/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0487 - acc: 0.9850 - val_loss: 2.0998 - val_acc: 0.6906\n",
      "Epoch 120/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2655 - acc: 0.9013 - val_loss: 1.1106 - val_acc: 0.7295\n",
      "Epoch 121/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1273 - acc: 0.9544 - val_loss: 6.6224 - val_acc: 0.5322\n",
      "Epoch 122/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0963 - acc: 0.9677 - val_loss: 0.4314 - val_acc: 0.8745\n",
      "Epoch 123/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0811 - acc: 0.9713 - val_loss: 1.1038 - val_acc: 0.7577\n",
      "Epoch 124/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0774 - acc: 0.9740 - val_loss: 0.7784 - val_acc: 0.8471\n",
      "Epoch 125/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0909 - acc: 0.9683 - val_loss: 0.8806 - val_acc: 0.7935\n",
      "Epoch 126/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0692 - acc: 0.9768 - val_loss: 0.5277 - val_acc: 0.8558\n",
      "Epoch 127/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0706 - acc: 0.9742 - val_loss: 0.8071 - val_acc: 0.8336\n",
      "Epoch 128/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0739 - acc: 0.9753 - val_loss: 0.7315 - val_acc: 0.8272\n",
      "Epoch 129/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0680 - acc: 0.9763 - val_loss: 0.8192 - val_acc: 0.8102\n",
      "Epoch 130/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0706 - acc: 0.9747 - val_loss: 0.6285 - val_acc: 0.8205\n",
      "Epoch 131/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1445 - acc: 0.9474 - val_loss: 0.8172 - val_acc: 0.8173\n",
      "Epoch 132/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0777 - acc: 0.9717 - val_loss: 0.9083 - val_acc: 0.8201\n",
      "Epoch 133/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0751 - acc: 0.9747 - val_loss: 1.9720 - val_acc: 0.6851\n",
      "Epoch 134/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0819 - acc: 0.9717 - val_loss: 2.4667 - val_acc: 0.6402\n",
      "Epoch 135/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0780 - acc: 0.9739 - val_loss: 5.1466 - val_acc: 0.5953\n",
      "Epoch 136/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0631 - acc: 0.9784 - val_loss: 0.6588 - val_acc: 0.8380\n",
      "Epoch 137/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0654 - acc: 0.9788 - val_loss: 1.3419 - val_acc: 0.7502\n",
      "Epoch 138/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0604 - acc: 0.9809 - val_loss: 0.5127 - val_acc: 0.8725\n",
      "Epoch 139/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0626 - acc: 0.9793 - val_loss: 1.7377 - val_acc: 0.7061\n",
      "Epoch 140/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1357 - acc: 0.9503 - val_loss: 0.8374 - val_acc: 0.7863\n",
      "Epoch 141/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0705 - acc: 0.9732 - val_loss: 1.0330 - val_acc: 0.8237\n",
      "Epoch 142/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0504 - acc: 0.9840 - val_loss: 3.2552 - val_acc: 0.6922\n",
      "Epoch 143/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0583 - acc: 0.9809 - val_loss: 5.7564 - val_acc: 0.5786\n",
      "Epoch 144/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0632 - acc: 0.9788 - val_loss: 4.8135 - val_acc: 0.6239\n",
      "Epoch 145/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0606 - acc: 0.9778 - val_loss: 0.6582 - val_acc: 0.8419\n",
      "Epoch 146/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1239 - acc: 0.9559 - val_loss: 0.3659 - val_acc: 0.8777\n",
      "Epoch 147/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1100 - acc: 0.9593 - val_loss: 2.3195 - val_acc: 0.5520\n",
      "Epoch 148/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0972 - acc: 0.9652 - val_loss: 1.2430 - val_acc: 0.8046\n",
      "Epoch 149/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0660 - acc: 0.9797 - val_loss: 0.8936 - val_acc: 0.7983\n",
      "Epoch 150/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0727 - acc: 0.9750 - val_loss: 4.3379 - val_acc: 0.6422\n",
      "Epoch 151/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0563 - acc: 0.9822 - val_loss: 2.2531 - val_acc: 0.5234\n",
      "Epoch 152/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0947 - acc: 0.9655 - val_loss: 0.5066 - val_acc: 0.8515\n",
      "Epoch 153/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1280 - acc: 0.9557 - val_loss: 2.1117 - val_acc: 0.7581\n",
      "Epoch 154/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0905 - acc: 0.9722 - val_loss: 0.7075 - val_acc: 0.8324\n",
      "Epoch 155/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1098 - acc: 0.9628 - val_loss: 2.0496 - val_acc: 0.7748\n",
      "Epoch 156/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0982 - acc: 0.9677 - val_loss: 4.1594 - val_acc: 0.6581\n",
      "Epoch 157/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0851 - acc: 0.9716 - val_loss: 0.4816 - val_acc: 0.8757\n",
      "Epoch 158/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0742 - acc: 0.9752 - val_loss: 0.4738 - val_acc: 0.8773\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0743 - acc: 0.9758 - val_loss: 3.2442 - val_acc: 0.7029\n",
      "Epoch 160/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0629 - acc: 0.9820 - val_loss: 0.8904 - val_acc: 0.8423\n",
      "Epoch 161/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0700 - acc: 0.9812 - val_loss: 0.5512 - val_acc: 0.8539\n",
      "Epoch 162/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0926 - acc: 0.9691 - val_loss: 0.6120 - val_acc: 0.8495\n",
      "Epoch 163/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0643 - acc: 0.9794 - val_loss: 2.7722 - val_acc: 0.7204\n",
      "Epoch 164/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0761 - acc: 0.9742 - val_loss: 0.6812 - val_acc: 0.8384\n",
      "Epoch 165/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0657 - acc: 0.9786 - val_loss: 1.4524 - val_acc: 0.8002\n",
      "Epoch 166/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0627 - acc: 0.9799 - val_loss: 1.9173 - val_acc: 0.7840\n",
      "Epoch 167/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0954 - acc: 0.9677 - val_loss: 0.6805 - val_acc: 0.8419\n",
      "Epoch 168/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0658 - acc: 0.9786 - val_loss: 0.6514 - val_acc: 0.8554\n",
      "Epoch 169/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0597 - acc: 0.9807 - val_loss: 0.6726 - val_acc: 0.8125\n",
      "Epoch 170/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0668 - acc: 0.9789 - val_loss: 0.8819 - val_acc: 0.7311\n",
      "Epoch 171/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0897 - acc: 0.9680 - val_loss: 0.5528 - val_acc: 0.8658\n",
      "Epoch 172/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0624 - acc: 0.9815 - val_loss: 0.7925 - val_acc: 0.8459\n",
      "Epoch 173/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0551 - acc: 0.9835 - val_loss: 0.7992 - val_acc: 0.8193\n",
      "Epoch 174/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0601 - acc: 0.9817 - val_loss: 1.1104 - val_acc: 0.8177\n",
      "Epoch 175/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0884 - acc: 0.9708 - val_loss: 3.9575 - val_acc: 0.6183\n",
      "Epoch 176/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0710 - acc: 0.9752 - val_loss: 1.8953 - val_acc: 0.6755\n",
      "Epoch 177/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0557 - acc: 0.9812 - val_loss: 0.4684 - val_acc: 0.8773\n",
      "Epoch 178/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0985 - acc: 0.9691 - val_loss: 0.6234 - val_acc: 0.8531\n",
      "Epoch 179/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0749 - acc: 0.9752 - val_loss: 0.8386 - val_acc: 0.8272\n",
      "Epoch 180/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0601 - acc: 0.9812 - val_loss: 0.5058 - val_acc: 0.8721\n",
      "Epoch 181/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0679 - acc: 0.9778 - val_loss: 1.5006 - val_acc: 0.7998\n",
      "Epoch 182/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0931 - acc: 0.9683 - val_loss: 0.9689 - val_acc: 0.7224\n",
      "Epoch 183/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0806 - acc: 0.9724 - val_loss: 0.6526 - val_acc: 0.8582\n",
      "Epoch 184/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0573 - acc: 0.9806 - val_loss: 0.8085 - val_acc: 0.7883\n",
      "Epoch 185/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0547 - acc: 0.9807 - val_loss: 0.9536 - val_acc: 0.7538\n",
      "Epoch 186/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0542 - acc: 0.9811 - val_loss: 0.5298 - val_acc: 0.8685\n",
      "Epoch 187/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0454 - acc: 0.9855 - val_loss: 0.8956 - val_acc: 0.8324\n",
      "Epoch 188/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0593 - acc: 0.9791 - val_loss: 0.6822 - val_acc: 0.8094\n",
      "Epoch 189/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0482 - acc: 0.9833 - val_loss: 0.8071 - val_acc: 0.7883\n",
      "Epoch 190/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0587 - acc: 0.9802 - val_loss: 0.7733 - val_acc: 0.8272\n",
      "Epoch 191/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0739 - acc: 0.9727 - val_loss: 0.6226 - val_acc: 0.8701\n",
      "Epoch 192/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0468 - acc: 0.9825 - val_loss: 0.8602 - val_acc: 0.8268\n",
      "Epoch 193/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1701 - acc: 0.9433 - val_loss: 0.9985 - val_acc: 0.8030\n",
      "Epoch 194/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0916 - acc: 0.9683 - val_loss: 0.7273 - val_acc: 0.8431\n",
      "Epoch 195/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0687 - acc: 0.9758 - val_loss: 0.6759 - val_acc: 0.8626\n",
      "Epoch 196/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0620 - acc: 0.9796 - val_loss: 0.5389 - val_acc: 0.8705\n",
      "Epoch 197/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0629 - acc: 0.9786 - val_loss: 0.7209 - val_acc: 0.8535\n",
      "Epoch 198/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0484 - acc: 0.9848 - val_loss: 0.6473 - val_acc: 0.8713\n",
      "Epoch 199/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0479 - acc: 0.9840 - val_loss: 0.6486 - val_acc: 0.8650\n",
      "Epoch 200/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0487 - acc: 0.9828 - val_loss: 0.5581 - val_acc: 0.8662\n",
      "2019-04-09 13:40:12.975145\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "batch_size =  16 #multiple of 1290/5 (recording length)\n",
    "nb_classes = 3\n",
    "rows, cols = 431, 128  \n",
    "nb_epoch = 200\n",
    "\n",
    "\n",
    "trainlist,validationlist,testlist=fileLists()\n",
    "print('lists sorted')\n",
    "X_tr, Y_train = getData(trainlist)\n",
    "X_v, Y_val = getData(validationlist)\n",
    "X_te, Y_test = getData(testlist)\n",
    "print('data obtained')\n",
    "X_train=np.expand_dims(X_tr,axis=3)\n",
    "X_val=np.expand_dims(X_v,axis=3)\n",
    "X_test=np.expand_dims(X_te,axis=3)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print('data sorted')\n",
    "model = compileCRNN(cols,rows,nb_classes=nb_classes)\n",
    "#model = load_model('models\\\\sceneModel200.hdf5')\n",
    "print('model compiled')\n",
    "savemodelfilename='models\\\\3groupedScenes.testsave'\n",
    "samples=(X_train.shape[0])\n",
    "print(datetime.now())\n",
    "models,histories= buildModel(savemodelfilename, samples,model,X_train,Y_train,X_val, Y_val)\n",
    "print(datetime.now())\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
