{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D,Dropout, Reshape, LSTM, BatchNormalization,TimeDistributed\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam #, RMSprop, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from time import time\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  16 #multiple of 1290/5 (recording length)\n",
    "nb_classes = 18\n",
    "rows, cols = 431, 128   \n",
    "nb_epoch = 200\n",
    "pool_size = (5,5)                  # size of pooling area for max pooling\n",
    "prob_drop_conv = 0.3                # drop probability for dropout @ conv layer\n",
    "prob_drop_hidden = 0.3              # drop probability for dropout @ fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data sorted\n"
     ]
    }
   ],
   "source": [
    "#might need to rewrite these\n",
    "\n",
    "\n",
    "\n",
    "def fileLists():\n",
    "    trainlist=[]\n",
    "    validationlist=[]\n",
    "    testlist=[]\n",
    "    evalSetupFiles='..\\\\..\\\\CASAdatasets\\\\DCASE18_ASCT1\\\\TUT-urban-acoustic-scenes-2018-development\\\\evaluation_setup\\\\*.txt'\n",
    "    txtfilelist=glob.glob(evalSetupFiles)\n",
    "    for txt in txtfilelist:\n",
    "        if '_location' not in txt:\n",
    "            continue\n",
    "        with open(txt,'r') as evaltxtfile:\n",
    "            for line in evaltxtfile.readlines():\n",
    "                line=line.strip().split('\\t')[0]\n",
    "                if 'train' in txt:\n",
    "                    trainlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "                elif 'test' in txt:\n",
    "                    testlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "                else:\n",
    "                    validationlist.append(line.replace('audio','logMelSpec').replace('.wav','_aggScenes.pckl').replace('/','\\\\'))\n",
    "    print('trainfiles: ', str(len(trainlist)))\n",
    "    print('validationfiles: ', str(len(validationlist)))\n",
    "    print('testfiles: ', str(len(testlist)))\n",
    "    return trainlist,validationlist,testlist\n",
    "\n",
    "labelRef={'inside-barcelona':0, 'outside-barcelona':1, 'transport-barcelona':2, \n",
    "          'inside-helsinki':3, 'outside-helsinki':4, 'transport-helsinki':5,\n",
    "          'inside-london':6, 'outside-london':7,'transport-london':8,\n",
    "          'inside-paris':9, 'outside-paris':10,'transport-paris':11,\n",
    "          'inside-stockholm':12, 'outside-stockholm':13,'transport-stockholm':14,\n",
    "          'inside-vienna':15, 'outside-vienna':16,'transport-vienna':17}\n",
    "\n",
    "def getData(flist):\n",
    "    pth='..\\\\..\\\\CASAdatasets\\\\DCASE18_ASCT1\\\\TUT-urban-acoustic-scenes-2018-development\\\\'\n",
    "    X_=np.zeros(((len(flist)),128,431))\n",
    "    Y_=np.zeros(((len(flist)),nb_classes))\n",
    "    for i,tfile in enumerate(flist):\n",
    "        with open(pth+tfile, \"rb\" ) as scenesample:\n",
    "            fv=pickle.load(scenesample)\n",
    "        X_[i,:,:]=fv\n",
    "        scene=''.join(tfile.split('\\\\')[1]).split('-')[0]\n",
    "        location=''.join(tfile.split('\\\\')[1]).split('-')[1]\n",
    "        if scene in ['airport','shopping_mall','metro_station']:\n",
    "            scene='inside-'+location\n",
    "        elif scene in ['street_pedestrian','public_square','street_traffic','park']:\n",
    "            scene='outside-'+location\n",
    "        else:\n",
    "            scene='transport-'+location\n",
    "        Y_[i,labelRef[scene]]=1\n",
    "    return X_, Y_\n",
    "#trainlist,validationlist,testlist=fileLists()\n",
    "#X_train, Y_train = getData(trainlist)\n",
    "#X_val, Y_val = getData(validationlist)\n",
    "#X_test, Y_test = getData(testlist)\n",
    "\n",
    "print(' data sorted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 431, 32)      1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 216, 64)       100416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 216, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 108, 128)      32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 54, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 110592)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                7077952   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 7,215,186\n",
      "Trainable params: 7,214,610\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###### Convolutional model\n",
    "def compileCRNN(cols,rows,nb_classes=1):\n",
    "    model = Sequential()\n",
    "    # conv1 layer\n",
    "    model.add(Conv2D(32, (7, 7), padding='same', activation='relu', input_shape=(cols,rows,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv2 layer\n",
    "    model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(4,7), strides=(2), padding='same'))\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    # conv3 layer\n",
    "    model.add(Conv2D(128, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, strides=(2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(prob_drop_conv))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # fc1 layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(prob_drop_hidden))\n",
    "    model.add(BatchNormalization())\n",
    "   \n",
    "    # fc2 layer\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = compileCRNN(cols,rows,nb_classes=nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "#fold=1\n",
    "def train_network(model, model_name, X_train, Y_train, X_val, Y_val, nb_epoch, validationsplit_size, batchsize, early_stoping_patience, output_folder):\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(output_folder,model_name + '.hdf5'),save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stoping_patience)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(time()))\n",
    "    Callbacks=[checkpointer,  tensorboard] #early_stopping,\n",
    "  #  print(samples)\n",
    "    steps=int(samples/batchsize)\n",
    "    validationsteps=int(validationsplit_size/batchsize)\n",
    "    history = model.fit(X_train, Y_train, epochs=nb_epoch, callbacks=Callbacks, batch_size=batch_size, validation_data=(X_val, Y_val), shuffle=True, verbose=1)\n",
    "    return history,model\n",
    "\n",
    "def buildModel(savemodelfilename, samples,model,X_train, Y_train,X_val, Y_val):\n",
    "    valSplit_size = int(samples/4)\n",
    "    early_stoping_patience=5\n",
    "    history,model = train_network(model, 'models\\\\groupedScenePairs', X_train, Y_train, X_val, Y_val, nb_epoch, valSplit_size, batch_size, early_stoping_patience,'.')\n",
    "    model.save_weights(savemodelfilename)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainfiles:  6122\n",
      "validationfiles:  2518\n",
      "testfiles:  2518\n",
      "lists sorted\n",
      "data obtained\n",
      "(6122, 128, 431, 1) (2518, 128, 431, 1) (2518, 128, 431, 1)\n",
      "data sorted\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 128, 431, 32)      1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128, 431, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64, 216, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 216, 64)       100416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 216, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 108, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 108, 128)      32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 54, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 110592)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                7077952   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 7,215,186\n",
      "Trainable params: 7,214,610\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "model compiled\n",
      "2019-04-09 14:55:02.497380\n",
      "Train on 6122 samples, validate on 2518 samples\n",
      "Epoch 1/200\n",
      "6122/6122 [==============================] - 26s 4ms/step - loss: 2.8421 - acc: 0.1292 - val_loss: 2.9530 - val_acc: 0.0596\n",
      "Epoch 2/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 2.5308 - acc: 0.2032 - val_loss: 3.1726 - val_acc: 0.0850\n",
      "Epoch 3/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 2.4449 - acc: 0.2310 - val_loss: 7.4809 - val_acc: 0.0373\n",
      "Epoch 4/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 2.3414 - acc: 0.2643 - val_loss: 2.8951 - val_acc: 0.1346\n",
      "Epoch 5/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 2.1384 - acc: 0.3465 - val_loss: 5.4398 - val_acc: 0.1152\n",
      "Epoch 6/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 2.0719 - acc: 0.3679 - val_loss: 5.3975 - val_acc: 0.1763\n",
      "Epoch 7/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.9616 - acc: 0.4008 - val_loss: 3.0153 - val_acc: 0.2478\n",
      "Epoch 8/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.8218 - acc: 0.4454 - val_loss: 2.8570 - val_acc: 0.1569\n",
      "Epoch 9/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.7715 - acc: 0.4562 - val_loss: 2.9686 - val_acc: 0.1338\n",
      "Epoch 10/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 1.6294 - acc: 0.5163 - val_loss: 2.2661 - val_acc: 0.2446\n",
      "Epoch 11/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.5657 - acc: 0.5271 - val_loss: 2.6093 - val_acc: 0.2268\n",
      "Epoch 12/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.4787 - acc: 0.5632 - val_loss: 2.4110 - val_acc: 0.1418\n",
      "Epoch 13/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.4170 - acc: 0.5753 - val_loss: 5.9124 - val_acc: 0.1076\n",
      "Epoch 14/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.3314 - acc: 0.6088 - val_loss: 2.5327 - val_acc: 0.2633\n",
      "Epoch 15/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.2630 - acc: 0.6317 - val_loss: 4.7590 - val_acc: 0.1672\n",
      "Epoch 16/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.2306 - acc: 0.6398 - val_loss: 5.1259 - val_acc: 0.1791\n",
      "Epoch 17/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.1310 - acc: 0.6632 - val_loss: 2.4532 - val_acc: 0.2585\n",
      "Epoch 18/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.0727 - acc: 0.6829 - val_loss: 2.4709 - val_acc: 0.2951\n",
      "Epoch 19/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.9786 - acc: 0.7192 - val_loss: 2.8860 - val_acc: 0.2442\n",
      "Epoch 20/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.9942 - acc: 0.7030 - val_loss: 2.4407 - val_acc: 0.3193\n",
      "Epoch 21/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8833 - acc: 0.7457 - val_loss: 2.7629 - val_acc: 0.3296\n",
      "Epoch 22/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.8348 - acc: 0.7571 - val_loss: 3.4850 - val_acc: 0.3054\n",
      "Epoch 23/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 1.0374 - acc: 0.6800 - val_loss: 2.7887 - val_acc: 0.2399\n",
      "Epoch 24/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.9560 - acc: 0.7104 - val_loss: 5.1783 - val_acc: 0.2355\n",
      "Epoch 25/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.9593 - acc: 0.7088 - val_loss: 4.6966 - val_acc: 0.1783\n",
      "Epoch 26/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.9072 - acc: 0.7145 - val_loss: 5.0725 - val_acc: 0.2585\n",
      "Epoch 27/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7910 - acc: 0.7592 - val_loss: 7.2299 - val_acc: 0.1446\n",
      "Epoch 28/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7062 - acc: 0.7948 - val_loss: 2.9548 - val_acc: 0.2538\n",
      "Epoch 29/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6880 - acc: 0.7937 - val_loss: 5.6039 - val_acc: 0.2351\n",
      "Epoch 30/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7911 - acc: 0.7504 - val_loss: 5.3710 - val_acc: 0.1843\n",
      "Epoch 31/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7591 - acc: 0.7679 - val_loss: 3.3568 - val_acc: 0.2756\n",
      "Epoch 32/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7725 - acc: 0.7689 - val_loss: 6.3353 - val_acc: 0.2292\n",
      "Epoch 33/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7632 - acc: 0.7738 - val_loss: 8.3858 - val_acc: 0.1974\n",
      "Epoch 34/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7102 - acc: 0.7947 - val_loss: 5.5140 - val_acc: 0.2442\n",
      "Epoch 35/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6944 - acc: 0.8048 - val_loss: 2.9767 - val_acc: 0.2931\n",
      "Epoch 36/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7327 - acc: 0.7885 - val_loss: 3.8913 - val_acc: 0.3193\n",
      "Epoch 37/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.7385 - acc: 0.7814 - val_loss: 8.0615 - val_acc: 0.2025\n",
      "Epoch 38/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6940 - acc: 0.8038 - val_loss: 2.6988 - val_acc: 0.2712\n",
      "Epoch 39/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6620 - acc: 0.8040 - val_loss: 4.5801 - val_acc: 0.2689\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6135 - acc: 0.8071 - val_loss: 2.5160 - val_acc: 0.3300\n",
      "Epoch 41/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4646 - acc: 0.8639 - val_loss: 3.3937 - val_acc: 0.3435\n",
      "Epoch 42/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4269 - acc: 0.8747 - val_loss: 2.8708 - val_acc: 0.3169\n",
      "Epoch 43/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4398 - acc: 0.8672 - val_loss: 2.5670 - val_acc: 0.3852\n",
      "Epoch 44/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4082 - acc: 0.8821 - val_loss: 2.2696 - val_acc: 0.4110\n",
      "Epoch 45/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.6698 - acc: 0.7885 - val_loss: 4.1054 - val_acc: 0.2216\n",
      "Epoch 46/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5410 - acc: 0.8298 - val_loss: 4.7450 - val_acc: 0.2649\n",
      "Epoch 47/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5927 - acc: 0.8175 - val_loss: 3.5169 - val_acc: 0.2784\n",
      "Epoch 48/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4693 - acc: 0.8607 - val_loss: 4.7273 - val_acc: 0.2685\n",
      "Epoch 49/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.5581 - acc: 0.8267 - val_loss: 2.0777 - val_acc: 0.4126\n",
      "Epoch 50/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.4359 - acc: 0.8711 - val_loss: 9.7514 - val_acc: 0.1966\n",
      "Epoch 51/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3773 - acc: 0.8942 - val_loss: 5.2900 - val_acc: 0.2967\n",
      "Epoch 52/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3653 - acc: 0.8964 - val_loss: 3.3950 - val_acc: 0.2923\n",
      "Epoch 53/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3660 - acc: 0.8966 - val_loss: 4.8627 - val_acc: 0.2919\n",
      "Epoch 54/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3417 - acc: 0.9010 - val_loss: 2.2664 - val_acc: 0.3967\n",
      "Epoch 55/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3197 - acc: 0.9097 - val_loss: 3.2879 - val_acc: 0.3344\n",
      "Epoch 56/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3146 - acc: 0.9120 - val_loss: 2.6886 - val_acc: 0.3701\n",
      "Epoch 57/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3152 - acc: 0.9146 - val_loss: 2.4965 - val_acc: 0.3932\n",
      "Epoch 58/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3561 - acc: 0.8977 - val_loss: 4.1820 - val_acc: 0.2502\n",
      "Epoch 59/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2836 - acc: 0.9141 - val_loss: 2.4381 - val_acc: 0.4087\n",
      "Epoch 60/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2702 - acc: 0.9234 - val_loss: 2.0284 - val_acc: 0.4297\n",
      "Epoch 61/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2898 - acc: 0.9162 - val_loss: 2.4000 - val_acc: 0.3733\n",
      "Epoch 62/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2856 - acc: 0.9190 - val_loss: 3.1739 - val_acc: 0.2971\n",
      "Epoch 63/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2813 - acc: 0.9219 - val_loss: 3.8936 - val_acc: 0.3689\n",
      "Epoch 64/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2992 - acc: 0.9131 - val_loss: 2.0872 - val_acc: 0.4480\n",
      "Epoch 65/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2675 - acc: 0.9188 - val_loss: 2.7420 - val_acc: 0.3670\n",
      "Epoch 66/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2314 - acc: 0.9334 - val_loss: 5.5496 - val_acc: 0.2760\n",
      "Epoch 67/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2305 - acc: 0.9301 - val_loss: 1.9694 - val_acc: 0.4801\n",
      "Epoch 68/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2280 - acc: 0.9324 - val_loss: 3.3884 - val_acc: 0.3570\n",
      "Epoch 69/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2144 - acc: 0.9352 - val_loss: 3.0493 - val_acc: 0.4325\n",
      "Epoch 70/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2088 - acc: 0.9405 - val_loss: 2.1553 - val_acc: 0.4305\n",
      "Epoch 71/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2285 - acc: 0.9332 - val_loss: 2.4719 - val_acc: 0.4047\n",
      "Epoch 72/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2468 - acc: 0.9226 - val_loss: 15.2471 - val_acc: 0.0488\n",
      "Epoch 73/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2632 - acc: 0.9187 - val_loss: 5.3943 - val_acc: 0.3415\n",
      "Epoch 74/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.3470 - acc: 0.8870 - val_loss: 2.6140 - val_acc: 0.3809\n",
      "Epoch 75/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2437 - acc: 0.9245 - val_loss: 2.5534 - val_acc: 0.4281\n",
      "Epoch 76/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2071 - acc: 0.9392 - val_loss: 2.7775 - val_acc: 0.4043\n",
      "Epoch 77/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2134 - acc: 0.9342 - val_loss: 2.4942 - val_acc: 0.4241\n",
      "Epoch 78/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1888 - acc: 0.9474 - val_loss: 2.3995 - val_acc: 0.4138\n",
      "Epoch 79/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2359 - acc: 0.9288 - val_loss: 2.6676 - val_acc: 0.4293\n",
      "Epoch 80/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.2032 - acc: 0.9407 - val_loss: 6.2497 - val_acc: 0.2975\n",
      "Epoch 81/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1698 - acc: 0.9513 - val_loss: 7.5470 - val_acc: 0.2415\n",
      "Epoch 82/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1964 - acc: 0.9422 - val_loss: 9.6112 - val_acc: 0.2149\n",
      "Epoch 83/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1605 - acc: 0.9543 - val_loss: 2.2511 - val_acc: 0.4531\n",
      "Epoch 84/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1704 - acc: 0.9477 - val_loss: 2.9801 - val_acc: 0.3515\n",
      "Epoch 85/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1539 - acc: 0.9548 - val_loss: 8.6455 - val_acc: 0.2391\n",
      "Epoch 86/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1416 - acc: 0.9577 - val_loss: 3.4681 - val_acc: 0.3165\n",
      "Epoch 87/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1496 - acc: 0.9569 - val_loss: 2.5776 - val_acc: 0.3860\n",
      "Epoch 88/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1364 - acc: 0.9600 - val_loss: 8.3063 - val_acc: 0.2748\n",
      "Epoch 89/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1636 - acc: 0.9528 - val_loss: 2.5815 - val_acc: 0.4345\n",
      "Epoch 90/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1395 - acc: 0.9597 - val_loss: 2.5680 - val_acc: 0.4492\n",
      "Epoch 91/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1426 - acc: 0.9577 - val_loss: 2.9604 - val_acc: 0.4245\n",
      "Epoch 92/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1348 - acc: 0.9606 - val_loss: 2.9575 - val_acc: 0.4146\n",
      "Epoch 93/200\n",
      "6122/6122 [==============================] - 25s 4ms/step - loss: 0.1381 - acc: 0.9593 - val_loss: 2.7834 - val_acc: 0.3483\n",
      "Epoch 94/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1241 - acc: 0.9650 - val_loss: 2.2142 - val_acc: 0.4881\n",
      "Epoch 95/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1337 - acc: 0.9613 - val_loss: 3.4112 - val_acc: 0.4166\n",
      "Epoch 96/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1467 - acc: 0.9562 - val_loss: 3.0502 - val_acc: 0.3320\n",
      "Epoch 97/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1681 - acc: 0.9495 - val_loss: 2.1981 - val_acc: 0.4670\n",
      "Epoch 98/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1554 - acc: 0.9544 - val_loss: 2.7143 - val_acc: 0.4114\n",
      "Epoch 99/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1427 - acc: 0.9587 - val_loss: 2.1891 - val_acc: 0.4651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3236 - acc: 0.9013 - val_loss: 6.3934 - val_acc: 0.2172\n",
      "Epoch 101/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2237 - acc: 0.9304 - val_loss: 2.8056 - val_acc: 0.3745\n",
      "Epoch 102/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1744 - acc: 0.9492 - val_loss: 3.7669 - val_acc: 0.4031\n",
      "Epoch 103/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1798 - acc: 0.9456 - val_loss: 2.5348 - val_acc: 0.4631\n",
      "Epoch 104/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1522 - acc: 0.9564 - val_loss: 3.2398 - val_acc: 0.3817\n",
      "Epoch 105/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1423 - acc: 0.9587 - val_loss: 2.1285 - val_acc: 0.4519\n",
      "Epoch 106/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1422 - acc: 0.9556 - val_loss: 2.9438 - val_acc: 0.4559\n",
      "Epoch 107/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1271 - acc: 0.9652 - val_loss: 3.1972 - val_acc: 0.3769\n",
      "Epoch 108/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1210 - acc: 0.9662 - val_loss: 2.4644 - val_acc: 0.4361\n",
      "Epoch 109/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1257 - acc: 0.9597 - val_loss: 2.2634 - val_acc: 0.4599\n",
      "Epoch 110/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1126 - acc: 0.9681 - val_loss: 13.8381 - val_acc: 0.0913\n",
      "Epoch 111/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1274 - acc: 0.9639 - val_loss: 2.5142 - val_acc: 0.4301\n",
      "Epoch 112/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1498 - acc: 0.9559 - val_loss: 2.3009 - val_acc: 0.4611\n",
      "Epoch 113/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1501 - acc: 0.9564 - val_loss: 5.5094 - val_acc: 0.1203\n",
      "Epoch 114/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3775 - acc: 0.8728 - val_loss: 3.3274 - val_acc: 0.3729\n",
      "Epoch 115/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1765 - acc: 0.9454 - val_loss: 2.1200 - val_acc: 0.4643\n",
      "Epoch 116/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1439 - acc: 0.9539 - val_loss: 2.9032 - val_acc: 0.4317\n",
      "Epoch 117/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1222 - acc: 0.9636 - val_loss: 2.4249 - val_acc: 0.4349\n",
      "Epoch 118/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1135 - acc: 0.9670 - val_loss: 2.7641 - val_acc: 0.4249\n",
      "Epoch 119/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1115 - acc: 0.9668 - val_loss: 2.3851 - val_acc: 0.4790\n",
      "Epoch 120/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1093 - acc: 0.9678 - val_loss: 2.2175 - val_acc: 0.4782\n",
      "Epoch 121/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0964 - acc: 0.9699 - val_loss: 2.1820 - val_acc: 0.4964\n",
      "Epoch 122/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0901 - acc: 0.9755 - val_loss: 2.4957 - val_acc: 0.4833\n",
      "Epoch 123/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1421 - acc: 0.9562 - val_loss: 2.5884 - val_acc: 0.3852\n",
      "Epoch 124/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1818 - acc: 0.9454 - val_loss: 2.7187 - val_acc: 0.4313\n",
      "Epoch 125/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1161 - acc: 0.9652 - val_loss: 2.3300 - val_acc: 0.4774\n",
      "Epoch 126/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1144 - acc: 0.9649 - val_loss: 2.3494 - val_acc: 0.4523\n",
      "Epoch 127/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1024 - acc: 0.9714 - val_loss: 3.5160 - val_acc: 0.4333\n",
      "Epoch 128/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0971 - acc: 0.9719 - val_loss: 1.9464 - val_acc: 0.5230\n",
      "Epoch 129/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1014 - acc: 0.9690 - val_loss: 2.5533 - val_acc: 0.3622\n",
      "Epoch 130/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1280 - acc: 0.9610 - val_loss: 3.2355 - val_acc: 0.3467\n",
      "Epoch 131/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.3401 - acc: 0.8959 - val_loss: 2.4790 - val_acc: 0.4607\n",
      "Epoch 132/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1700 - acc: 0.9459 - val_loss: 8.2203 - val_acc: 0.2303\n",
      "Epoch 133/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1212 - acc: 0.9628 - val_loss: 5.0951 - val_acc: 0.3606\n",
      "Epoch 134/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1097 - acc: 0.9680 - val_loss: 2.0645 - val_acc: 0.4658\n",
      "Epoch 135/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1136 - acc: 0.9636 - val_loss: 2.1979 - val_acc: 0.4865\n",
      "Epoch 136/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0989 - acc: 0.9727 - val_loss: 1.9794 - val_acc: 0.4877\n",
      "Epoch 137/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0907 - acc: 0.9739 - val_loss: 2.2138 - val_acc: 0.4746\n",
      "Epoch 138/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0900 - acc: 0.9719 - val_loss: 5.8803 - val_acc: 0.3058\n",
      "Epoch 139/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0951 - acc: 0.9709 - val_loss: 2.2325 - val_acc: 0.4758\n",
      "Epoch 140/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.1065 - acc: 0.9680 - val_loss: 2.0977 - val_acc: 0.4519\n",
      "Epoch 141/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0893 - acc: 0.9745 - val_loss: 2.7559 - val_acc: 0.4182\n",
      "Epoch 142/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0945 - acc: 0.9690 - val_loss: 2.4672 - val_acc: 0.4186\n",
      "Epoch 143/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1046 - acc: 0.9691 - val_loss: 3.1408 - val_acc: 0.3733\n",
      "Epoch 144/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0894 - acc: 0.9716 - val_loss: 2.9512 - val_acc: 0.4186\n",
      "Epoch 145/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0887 - acc: 0.9737 - val_loss: 4.6715 - val_acc: 0.3046\n",
      "Epoch 146/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0980 - acc: 0.9691 - val_loss: 2.4022 - val_acc: 0.4591\n",
      "Epoch 147/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0854 - acc: 0.9729 - val_loss: 2.7340 - val_acc: 0.4408\n",
      "Epoch 148/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0717 - acc: 0.9770 - val_loss: 2.5061 - val_acc: 0.4444\n",
      "Epoch 149/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1049 - acc: 0.9673 - val_loss: 2.6039 - val_acc: 0.4313\n",
      "Epoch 150/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0774 - acc: 0.9757 - val_loss: 3.4711 - val_acc: 0.4095\n",
      "Epoch 151/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0787 - acc: 0.9771 - val_loss: 3.6016 - val_acc: 0.4222\n",
      "Epoch 152/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0870 - acc: 0.9730 - val_loss: 2.1275 - val_acc: 0.4952\n",
      "Epoch 153/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0845 - acc: 0.9747 - val_loss: 6.1977 - val_acc: 0.3364\n",
      "Epoch 154/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1021 - acc: 0.9667 - val_loss: 2.5608 - val_acc: 0.4658\n",
      "Epoch 155/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1250 - acc: 0.9608 - val_loss: 11.4955 - val_acc: 0.0977\n",
      "Epoch 156/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2811 - acc: 0.9124 - val_loss: 2.8331 - val_acc: 0.4059\n",
      "Epoch 157/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.2186 - acc: 0.9298 - val_loss: 2.2394 - val_acc: 0.4670\n",
      "Epoch 158/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1226 - acc: 0.9616 - val_loss: 2.5262 - val_acc: 0.4861\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0981 - acc: 0.9680 - val_loss: 2.2438 - val_acc: 0.4857\n",
      "Epoch 160/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0938 - acc: 0.9713 - val_loss: 5.5561 - val_acc: 0.3713\n",
      "Epoch 161/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0948 - acc: 0.9709 - val_loss: 2.3773 - val_acc: 0.4464\n",
      "Epoch 162/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1920 - acc: 0.9378 - val_loss: 4.6710 - val_acc: 0.2077\n",
      "Epoch 163/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1921 - acc: 0.9371 - val_loss: 2.5155 - val_acc: 0.4595\n",
      "Epoch 164/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1152 - acc: 0.9654 - val_loss: 3.0455 - val_acc: 0.4527\n",
      "Epoch 165/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0985 - acc: 0.9703 - val_loss: 2.4032 - val_acc: 0.4567\n",
      "Epoch 166/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1280 - acc: 0.9616 - val_loss: 3.5423 - val_acc: 0.4448\n",
      "Epoch 167/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0907 - acc: 0.9729 - val_loss: 3.5754 - val_acc: 0.4384\n",
      "Epoch 168/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1011 - acc: 0.9667 - val_loss: 2.6441 - val_acc: 0.4237\n",
      "Epoch 169/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0770 - acc: 0.9778 - val_loss: 2.4195 - val_acc: 0.4766\n",
      "Epoch 170/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0824 - acc: 0.9747 - val_loss: 3.6211 - val_acc: 0.4376\n",
      "Epoch 171/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0721 - acc: 0.9796 - val_loss: 2.7773 - val_acc: 0.4396\n",
      "Epoch 172/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0892 - acc: 0.9726 - val_loss: 4.0702 - val_acc: 0.4130\n",
      "Epoch 173/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0836 - acc: 0.9745 - val_loss: 2.3301 - val_acc: 0.4762\n",
      "Epoch 174/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0762 - acc: 0.9781 - val_loss: 3.6707 - val_acc: 0.4257\n",
      "Epoch 175/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0710 - acc: 0.9796 - val_loss: 2.7009 - val_acc: 0.4897\n",
      "Epoch 176/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0749 - acc: 0.9768 - val_loss: 4.8439 - val_acc: 0.4051\n",
      "Epoch 177/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0815 - acc: 0.9752 - val_loss: 14.4213 - val_acc: 0.0766\n",
      "Epoch 178/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0834 - acc: 0.9737 - val_loss: 2.6008 - val_acc: 0.4424\n",
      "Epoch 179/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1161 - acc: 0.9646 - val_loss: 2.9065 - val_acc: 0.3824\n",
      "Epoch 180/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0826 - acc: 0.9744 - val_loss: 2.5279 - val_acc: 0.4702\n",
      "Epoch 181/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0771 - acc: 0.9783 - val_loss: 2.3849 - val_acc: 0.4853\n",
      "Epoch 182/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0650 - acc: 0.9804 - val_loss: 2.4969 - val_acc: 0.4639\n",
      "Epoch 183/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0597 - acc: 0.9814 - val_loss: 4.4134 - val_acc: 0.3046\n",
      "Epoch 184/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0684 - acc: 0.9817 - val_loss: 2.5465 - val_acc: 0.4631\n",
      "Epoch 185/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0634 - acc: 0.9794 - val_loss: 2.7392 - val_acc: 0.4682\n",
      "Epoch 186/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0749 - acc: 0.9760 - val_loss: 2.7124 - val_acc: 0.4305\n",
      "Epoch 187/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0720 - acc: 0.9770 - val_loss: 2.7512 - val_acc: 0.4504\n",
      "Epoch 188/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0581 - acc: 0.9824 - val_loss: 2.4499 - val_acc: 0.4805\n",
      "Epoch 189/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0616 - acc: 0.9794 - val_loss: 2.8967 - val_acc: 0.4261\n",
      "Epoch 190/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0662 - acc: 0.9806 - val_loss: 2.7754 - val_acc: 0.3904\n",
      "Epoch 191/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0991 - acc: 0.9722 - val_loss: 2.9643 - val_acc: 0.4412\n",
      "Epoch 192/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0865 - acc: 0.9735 - val_loss: 2.4285 - val_acc: 0.4786\n",
      "Epoch 193/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.1154 - acc: 0.9637 - val_loss: 2.8043 - val_acc: 0.4416\n",
      "Epoch 194/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0802 - acc: 0.9739 - val_loss: 3.3997 - val_acc: 0.3666\n",
      "Epoch 195/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0694 - acc: 0.9768 - val_loss: 5.5383 - val_acc: 0.3590\n",
      "Epoch 196/200\n",
      "6122/6122 [==============================] - 23s 4ms/step - loss: 0.0589 - acc: 0.9833 - val_loss: 2.2320 - val_acc: 0.5020\n",
      "Epoch 197/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0585 - acc: 0.9815 - val_loss: 2.8928 - val_acc: 0.4031\n",
      "Epoch 198/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0574 - acc: 0.9832 - val_loss: 3.0884 - val_acc: 0.4579\n",
      "Epoch 199/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0621 - acc: 0.9793 - val_loss: 2.3676 - val_acc: 0.5008\n",
      "Epoch 200/200\n",
      "6122/6122 [==============================] - 24s 4ms/step - loss: 0.0630 - acc: 0.9799 - val_loss: 6.5384 - val_acc: 0.3380\n",
      "2019-04-09 16:14:27.889800\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "batch_size =  16 #multiple of 1290/5 (recording length)\n",
    "nb_classes = 18\n",
    "rows, cols = 431, 128  \n",
    "\n",
    "\n",
    "trainlist,validationlist,testlist=fileLists()\n",
    "print('lists sorted')\n",
    "X_tr, Y_train = getData(trainlist)\n",
    "X_v, Y_val = getData(validationlist)\n",
    "X_te, Y_test = getData(testlist)\n",
    "print('data obtained')\n",
    "X_train=np.expand_dims(X_tr,axis=3)\n",
    "X_val=np.expand_dims(X_v,axis=3)\n",
    "X_test=np.expand_dims(X_te,axis=3)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print('data sorted')\n",
    "model = compileCRNN(cols,rows,nb_classes=nb_classes)\n",
    "#model=load_model('models\\\\best_model60_simple.hdf5')\n",
    "print('model compiled')\n",
    "savemodelfilename='models\\\\groupedScenePairs.testsave'\n",
    "samples=(X_train.shape[0])\n",
    "print(datetime.now())\n",
    "models,histories= buildModel(savemodelfilename, samples,model,X_train,Y_train,X_val, Y_val)\n",
    "print(datetime.now())\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
